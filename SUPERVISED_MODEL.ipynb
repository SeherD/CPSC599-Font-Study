{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mc651XAM0Xwi"
      },
      "source": [
        "Data Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujWVCX920Xwm"
      },
      "source": [
        "Dataset builder- Supervised"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6zZVhUCht766"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TdmMjXtf0Xwm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def word_to_tensor(word):\n",
        "    # Convert the word to lowercase\n",
        "    word = word.lower()\n",
        "\n",
        "    # Use a fixed size of 26 for the alphabet\n",
        "    letter_count = [0] * 26\n",
        "\n",
        "    for char in word:\n",
        "        if 'a' <= char <= 'z':\n",
        "            letter_count[ord(char) - ord('a')] = 1\n",
        "\n",
        "    return torch.tensor(letter_count, dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kS3oxLw0Xwm",
        "outputId": "6436c16f-c640-4af4-9d14-55175dc2601b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a: 552\n",
            "b: 157\n",
            "c: 284\n",
            "d: 233\n",
            "e: 613\n",
            "f: 85\n",
            "g: 183\n",
            "h: 211\n",
            "i: 540\n",
            "j: 11\n",
            "k: 86\n",
            "l: 393\n",
            "m: 240\n",
            "n: 451\n",
            "o: 434\n",
            "p: 234\n",
            "q: 10\n",
            "r: 494\n",
            "s: 355\n",
            "t: 423\n",
            "u: 293\n",
            "v: 73\n",
            "w: 70\n",
            "x: 32\n",
            "y: 170\n",
            "z: 33\n",
            "Class Weights: tensor([0.0697, 0.2450, 0.1354, 0.1651, 0.0627, 0.4525, 0.2102, 0.1823, 0.0712,\n",
            "        3.4965, 0.4472, 0.0979, 0.1603, 0.0853, 0.0886, 0.1644, 3.8462, 0.0779,\n",
            "        0.1083, 0.0909, 0.1313, 0.5269, 0.5495, 1.2019, 0.2262, 1.1655])\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import csv\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "class SupervisedDataset(Dataset):\n",
        "    def __init__(self, root_dir, labels_path, transform = None):\n",
        "        self.root_dir = root_dir\n",
        "        self.labels_path = labels_path\n",
        "        self.data = []\n",
        "        self.transform = transform\n",
        "        with open(labels_path, newline=\"\") as labels_file:\n",
        "            labels_reader = csv.reader(labels_file)\n",
        "            for row in labels_reader:\n",
        "                self.data.append(row)  # a list of [filename, [chars in image]]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.root_dir, self.data[idx][0])\n",
        "\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image=self.transform(image)\n",
        "        label_text = self.data[idx][1]\n",
        "\n",
        "        # Convert label text to array of letter counts\n",
        "        label_tensor = word_to_tensor(label_text)\n",
        "\n",
        "        return image, label_tensor\n",
        "\n",
        "# Define the transform\n",
        "transform = transforms.Compose([\n",
        "\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "    transforms.RandomRotation(20),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Set your root directory\n",
        "root_dir = \"supervised\"\n",
        "# subfolder = 'supervised_data'\n",
        "\n",
        "# Create datasets for each model\n",
        "models = [\n",
        "    \"arial\",\n",
        "    \"bradhitc\",\n",
        "    \"century_schoolbook\",\n",
        "    \"comic\",\n",
        "    \"cour\",\n",
        "    \"papyrus\",\n",
        "    \"times\",\n",
        "]\n",
        "train_datasets, val_datasets = [], []\n",
        "\n",
        "for model in models:\n",
        "    model_dir = os.path.join(root_dir, f\"{model}_images\")\n",
        "    labels_path = os.path.join(root_dir, f\"{model}.csv\")\n",
        "    all_data = SupervisedDataset(model_dir, labels_path, transform)\n",
        "\n",
        "    # Split data into training and validation sets\n",
        "    train_size = 1000\n",
        "    val_size = 100\n",
        "    train_data, val_data = torch.utils.data.random_split(\n",
        "        all_data, [train_size, val_size]\n",
        "    )\n",
        "\n",
        "    train_datasets.append(train_data)\n",
        "    val_datasets.append(val_data)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loaders = [\n",
        "    DataLoader(dataset, batch_size=32, shuffle=True) for dataset in train_datasets\n",
        "]\n",
        "val_loaders = [\n",
        "    DataLoader(dataset, batch_size=32, shuffle=False) for dataset in val_datasets\n",
        "]\n",
        "\n",
        "\n",
        "letter_counts = {chr(ord('a') + i): 0 for i in range(26)}\n",
        "\n",
        "# Iterate through the dataset and update letter counts\n",
        "for _, label in train_datasets[0]:\n",
        "    for i, value in enumerate(label):\n",
        "        if value == 1:\n",
        "            letter = chr(ord('a') + i)\n",
        "            letter_counts[letter] += 1\n",
        "\n",
        "# Print the counts for each letter\n",
        "for letter, count in letter_counts.items():\n",
        "    print(f\"{letter}: {count}\")\n",
        "\n",
        "class_counts = torch.tensor([letter_counts[letter] for letter in sorted(letter_counts.keys())], dtype=torch.float32)\n",
        "\n",
        "# Calculate class weights\n",
        "total_samples = len(train_datasets[0])\n",
        "class_weights = total_samples / (26 * class_counts)\n",
        "\n",
        "# Print the calculated class weights\n",
        "print(\"Class Weights:\", class_weights)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPZNRupU0Xwn"
      },
      "source": [
        "Dataset builder - Unsupervised"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lJFJUkovEba"
      },
      "outputs": [],
      "source": [
        "# Stacked Convolutional Auto-Encoder (the unsupervised sub-network)\n",
        "class SCAE(nn.Module):\n",
        "  def __init__(self, num_channels):\n",
        "    super().__init__()\n",
        "\n",
        "    self.conv1 = nn.Conv2d(\n",
        "        in_channels=num_channels,\n",
        "        out_channels=64,\n",
        "        kernel_size=11,\n",
        "        padding=1,\n",
        "        stride=2\n",
        "    )\n",
        "    self.conv2 = nn.Conv2d(\n",
        "        in_channels=64,\n",
        "        out_channels=128,\n",
        "        kernel_size=5,\n",
        "        padding=2\n",
        "    )\n",
        "    self.deconv1 = nn.ConvTranspose2d(\n",
        "        in_channels = 128,\n",
        "        out_channels = 64,\n",
        "        kernel_size = 5,\n",
        "        padding = 2\n",
        "    )\n",
        "    self.deconv2 = nn.ConvTranspose2d(\n",
        "        in_channels=64,\n",
        "        out_channels=3,\n",
        "        kernel_size=11,\n",
        "        padding=1,\n",
        "        # using stride in the conv1 layer means that multiple input sizes are mapped to the same size\n",
        "        # output_padding of 1 ensures that the output is the same size as the input\n",
        "        # in the specific case that the model is producing an output 1 smaller than the input in both dimensions\n",
        "        # change the output padding value if you change the input image size\n",
        "        output_padding=1,\n",
        "        stride=2\n",
        "    )\n",
        "    self.maxpool = nn.MaxPool2d(2, return_indices=True)\n",
        "    self.unpool = nn.MaxUnpool2d(2)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    x1 = self.conv1(x)\n",
        "    x2 = self.relu(x1)\n",
        "    x3, indices = self.maxpool(x2)\n",
        "\n",
        "    x4 = self.conv2(x3)\n",
        "    x5 = self.relu(x4)\n",
        "\n",
        "    x6 = self.deconv1(x5)\n",
        "    x7 = self.unpool(x6, indices, output_size=x2.size())\n",
        "    x8 = self.relu(x7)\n",
        "\n",
        "    x9 = self.deconv2(x8)\n",
        "    x10 = self.relu(x9)\n",
        "\n",
        "    return x10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "PtnGJaRkS1L5"
      },
      "outputs": [],
      "source": [
        "class DeepFont(nn.Module):\n",
        "  def __init__(self, num_channels, num_classes):\n",
        "    super().__init__()\n",
        "\n",
        "    self.baby = nn.Linear(in_features=256*256*num_channels, out_features=num_classes)\n",
        "\n",
        "    self.conv1 = nn.Conv2d(\n",
        "        in_channels=num_channels,\n",
        "        out_channels=64,\n",
        "        kernel_size=11,\n",
        "        padding=1,\n",
        "        stride=2\n",
        "    )\n",
        "    self.conv2 = nn.Conv2d(\n",
        "        in_channels=64,\n",
        "        out_channels=128,\n",
        "        kernel_size=5,\n",
        "        padding=2\n",
        "    )\n",
        "    self.conv3 = nn.Conv2d(\n",
        "        in_channels=128,\n",
        "        out_channels=256,\n",
        "        kernel_size=3,\n",
        "        padding=1\n",
        "    )\n",
        "    self.conv4 = nn.Conv2d(\n",
        "        in_channels=256,\n",
        "        out_channels=256,\n",
        "        kernel_size=3,\n",
        "        padding=1\n",
        "    )\n",
        "    self.conv5 = nn.Conv2d(\n",
        "        in_channels=256,\n",
        "        out_channels=256,\n",
        "        kernel_size=3,\n",
        "        padding=1\n",
        "    )\n",
        "    self.fc6 = nn.Linear(in_features=31*31*256, out_features=4096) # assuming input image size of 256x256. change in_feats for different sample size\n",
        "    self.fc7 = nn.Linear(in_features=4096, out_features=4096)\n",
        "    self.fc8 = nn.Linear(in_features=4096, out_features=num_classes)\n",
        "    self.norm1 = nn.BatchNorm2d(num_features=64)\n",
        "    self.norm2 = nn.BatchNorm2d(num_features=128)\n",
        "    self.dropout = nn.Dropout(0.5)\n",
        "    self.maxpool = nn.MaxPool2d(2)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.flatten = nn.Flatten()\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x = self.flatten(x)\n",
        "    # x = self.baby(x)\n",
        "\n",
        "    x = self.conv1(x)\n",
        "    x = self.norm1(x)\n",
        "    x = self.maxpool(x)\n",
        "    x = self.relu(x)\n",
        "\n",
        "    x = self.conv2(x)\n",
        "    x = self.norm2(x)\n",
        "    x = self.maxpool(x)\n",
        "    x = self.relu(x)\n",
        "\n",
        "    x = self.conv3(x)\n",
        "    x = self.relu(x)\n",
        "\n",
        "    x = self.conv4(x)\n",
        "    x = self.relu(x)\n",
        "\n",
        "    x = self.conv5(x)\n",
        "    x = self.relu(x)\n",
        "\n",
        "    x = self.flatten(x)\n",
        "\n",
        "    x = self.dropout(self.fc6(x))\n",
        "    x = self.relu(x)\n",
        "\n",
        "    x = self.dropout(self.fc7(x))\n",
        "    x = self.relu(x)\n",
        "\n",
        "    x = self.fc8(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "keQLHsZB0Xwp"
      },
      "outputs": [],
      "source": [
        "def training_unsupervised(model, dataloader, criterion, optimizer, device, epochs, model_path):\n",
        "    model = model.to(device)\n",
        "    model.train()\n",
        "    best_loss = torch.inf\n",
        "    for _ in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch_index, (images, _) in enumerate(dataloader):\n",
        "            optimizer.zero_grad()\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, images)\n",
        "            total_loss += loss.item() * images.size(0)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        avg_loss = total_loss / (batch_index+1)\n",
        "        if avg_loss < best_loss:\n",
        "            best_loss = avg_loss\n",
        "            torch.save(model.state_dict(), model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "CyIpyIs-0Xwp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def evaluation(model, dataloader, criterion, device, phase='Validation'):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    ground_truth = []\n",
        "\n",
        "    true_positives = 0\n",
        "    true_negatives = 0\n",
        "    false_positives = 0\n",
        "    false_negatives = 0\n",
        "\n",
        "    misclassified_examples = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        total_loss = 0\n",
        "        total_samples = 0\n",
        "\n",
        "        for _, (images, labels) in enumerate(dataloader):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            #print(outputs)\n",
        "            #print(labels)\n",
        "            loss = criterion(outputs, labels)\n",
        "            #print(loss)\n",
        "            total_loss += loss.item() * images.size(0)\n",
        "            total_samples += images.size(0)\n",
        "\n",
        "            # Convert output probabilities to binary predictions\n",
        "            preds = (torch.sigmoid(outputs) > 0.5).float()\n",
        "\n",
        "            # Update multi-label metrics\n",
        "            true_positives += (preds * labels).sum().item()\n",
        "            true_negatives += ((1 - labels) * (1 - preds)).sum().item()\n",
        "            false_positives += ((1 - labels) * preds).sum().item()\n",
        "            false_negatives += (labels * (1 - preds)).sum().item()\n",
        "\n",
        "            # Collect misclassified examples\n",
        "\n",
        "\n",
        "            predictions.extend(preds.cpu().numpy())\n",
        "            ground_truth.extend(labels.cpu().numpy())\n",
        "\n",
        "        # Calculate multi-label metrics\n",
        "        precision = true_positives / (true_positives + false_positives + 1e-10)\n",
        "        recall = true_positives / (true_positives + false_negatives + 1e-10)\n",
        "        f1_score = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
        "\n",
        "        accuracy = (true_positives + true_negatives) / (total_samples + 1e-10)\n",
        "        loss = total_loss / total_samples\n",
        "\n",
        "        print(f'{phase}\\tF1-Score={f1_score:<10.4f}' +\n",
        "              f'\\t\\tLoss= {loss:<10.4f}' +\n",
        "              f'\\t\\tPrecision: {precision:<10.4f}' +\n",
        "              f'\\t\\tRecall: {recall:<10.4f}' +\n",
        "              f'\\t\\tAccuracy: {accuracy:<10.4f}')\n",
        "\n",
        "        return {'loss': loss,\n",
        "                'f1_score': f1_score,\n",
        "                'precision': precision,\n",
        "                'recall': recall,\n",
        "                'accuracy': accuracy,\n",
        "                'ground_truth': ground_truth,\n",
        "                'predictions': predictions}\n",
        "\n",
        "# Example usage:\n",
        "# Replace 'your_model' and 'your_dataloader' with your actual model and dataloader\n",
        "# Replace 'your_device' with 'cuda' or 'cpu' depending on your setup\n",
        "# evaluation_results = evaluation(your_model, your_dataloader, criterion, your_device)\n",
        "# misclassified_examples = evaluation_results['misclassified_examples']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Cns4YsEL0Xwq"
      },
      "outputs": [],
      "source": [
        "from torch.optim import lr_scheduler\n",
        "\n",
        "def training_supervised(model, train_loader, val_loader, criterion, optimizer, scheduler, device, epochs, best_model_path):\n",
        "    model = model.to(device)\n",
        "    model.train()\n",
        "    best_loss = torch.inf\n",
        "    best_results = None\n",
        "    youre_on_thin_ice_buster = False\n",
        "    #misclassified_examples = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        total_samples = 0\n",
        "\n",
        "        # New variables for multi-label metrics\n",
        "        true_positives = 0\n",
        "        true_negatives = 0\n",
        "        false_positives = 0\n",
        "        false_negatives = 0\n",
        "\n",
        "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item() * images.size(0)\n",
        "            total_samples += images.size(0)\n",
        "\n",
        "            # Convert output probabilities to binary predictions\n",
        "            preds = (torch.sigmoid(outputs) > 0.5).float()\n",
        "\n",
        "            # Update multi-label metrics\n",
        "            true_positives += (preds * labels).sum().item()\n",
        "            true_negatives += ((1 - labels) * (1 - preds)).sum().item()\n",
        "            false_positives += ((1 - labels) * preds).sum().item()\n",
        "            false_negatives += (labels * (1 - preds)).sum().item()\n",
        "\n",
        "        #if scheduler:\n",
        "            #scheduler.step()\n",
        "\n",
        "\n",
        "        # Calculate multi-label metrics\n",
        "        precision = true_positives / (true_positives + false_positives + 1e-10)\n",
        "        recall = true_positives / (true_positives + false_negatives + 1e-10)\n",
        "        f1_score = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
        "\n",
        "        accuracy = (true_positives + true_negatives) / (total_samples + 1e-10)\n",
        "        loss = total_loss / total_samples\n",
        "\n",
        "        print(f'{epoch:<4}\\tTrain\\tF1-Score={f1_score:<10.4f}' +\n",
        "              f'\\t\\tLoss= {loss:<10.4f}' +\n",
        "              f'\\t\\tPrecision: {precision:<10.4f}' +\n",
        "              f'\\t\\tRecall: {recall:<10.4f}' +\n",
        "              f'\\t\\tAccuracy: {accuracy:<10.4f}')\n",
        "\n",
        "        results = evaluation(model, val_loader, criterion, device)\n",
        "        model.train()\n",
        "\n",
        "        # early stopping:\n",
        "        if results['loss'] < best_loss:             # we are still improving\n",
        "            torch.save(model.state_dict(), best_model_path)\n",
        "            best_loss = results['loss']\n",
        "            best_results = results\n",
        "            youre_on_thin_ice_buster = False\n",
        "        elif youre_on_thin_ice_buster:              # we didn't improve last time and we didn't improve this time\n",
        "            break\n",
        "        else:                                       # we didn't improve this time, but it was the first time in a while\n",
        "            youre_on_thin_ice_buster = True\n",
        "\n",
        "        #if epoch == epochs - 1:  # Check if it's the last epoch\n",
        "         # misclassified_examples = find_misclassified_examples(model, val_loader, device)\n",
        "          #print(\"Misclassified Examples:\")\n",
        "          #for example in misclassified_examples:\n",
        "           #   print(example)\n",
        "\n",
        "        for name, param in model.named_parameters():\n",
        "              if param.requires_grad and param.grad is not None:\n",
        "               # print(f\"Layer: {name}, Gradient Norm: {param.grad.norm().item()}\")\n",
        "               pass\n",
        "        print()\n",
        "\n",
        "\n",
        "    # Print misclassified examples after the last epoch\n",
        "\n",
        "    # Print misclassified examples after the last epoch\n",
        "    #print(\"Misclassified Examples:\")\n",
        "    #for example in misclassified_examples:\n",
        "     #   print(example)\n",
        "\n",
        "    return best_results\n",
        "\n",
        "def find_misclassified_examples(model, data_loader, device):\n",
        "    model.eval()\n",
        "    misclassified_examples = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in data_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            preds = (torch.sigmoid(outputs) > 0.5).float()\n",
        "\n",
        "            misclassified_mask = (preds != labels)\n",
        "            misclassified_indices = torch.nonzero(misclassified_mask).squeeze()\n",
        "\n",
        "            for idx in misclassified_indices:\n",
        "                # Append to misclassified examples without moving to CPU\n",
        "                misclassified_examples.append({\n",
        "                    'image': images[idx].clone(),  # Use clone to avoid modifying the original tensor\n",
        "                    'predicted_label': preds[idx].clone(),\n",
        "                    'true_label': labels[idx].clone()\n",
        "                })\n",
        "\n",
        "    return misclassified_examples\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qC-mt2PuRmjD",
        "outputId": "1a63b229-f319-445b-c1e0-004fa18ca611"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1000\n"
          ]
        }
      ],
      "source": [
        "print(len(train_datasets[0]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "l2trhGSP0Xwr"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "models_dir = 'models'\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "# Train the unsupervised sub-network IS NOT IN THIS NOTEBOOK ANYMORE GO SEE OTHER NOTEBOOK\n",
        "\n",
        "\n",
        "# Train the supervised sub-network\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.0001\n",
        "momentum = 0.95\n",
        "weight_decay = 1e-4\n",
        "epochs = 16 # CHANGED BY XANNA \n",
        "criterion = torch.nn.BCEWithLogitsLoss(weight=class_weights.to(device))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stops colab from breaking sometimes\n",
        "# Only works sometimes\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 720
        },
        "id": "rgk8Xxqb0Xwr",
        "outputId": "59a8f9c6-7395-402a-c3db-ed586f0b7d7c"
      },
      "outputs": [],
      "source": [
        "# define supervised model\n",
        "supervised_model = DeepFont(\n",
        "    num_channels=3, num_classes=26\n",
        ")  # one class per letter (not case-sensitive)\n",
        "\n",
        "# Import the convolutional layers of the SCAE as conv1 and conv2\n",
        "scae_path = os.path.join(models_dir, f\"SCAE.pt\")\n",
        "supervised_model.load_state_dict(torch.load(scae_path), strict=False)\n",
        "\n",
        "# Freeze the convolutional layers from SCAE\n",
        "for param in supervised_model.conv1.parameters():\n",
        "    param.requires_grad = False\n",
        "for param in supervised_model.conv2.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# unfreeze layers                                   # something something when i removed coconunt.jpg the whole project broke and we don;t even wanna know if this is a coconut\n",
        "for param in supervised_model.conv3.parameters():\n",
        "    param.requires_grad = True\n",
        "for param in supervised_model.conv4.parameters():\n",
        "    param.requires_grad = True\n",
        "for param in supervised_model.conv5.parameters():\n",
        "    param.requires_grad = True\n",
        "for param in supervised_model.fc6.parameters():\n",
        "    param.requires_grad = True\n",
        "for param in supervised_model.fc7.parameters():\n",
        "    param.requires_grad = True\n",
        "for param in supervised_model.fc8.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# define optimizer and scheduler :) thank u xanna. ur welcome\n",
        "optimizer = optim.SGD(supervised_model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "arial\n",
            "0   \tTrain\tF1-Score=0.3593    \t\tLoss= 0.2399    \t\tPrecision: 0.3497    \t\tRecall: 0.3696    \t\tAccuracy: 17.2500   \n",
            "Validation\tF1-Score=0.3452    \t\tLoss= 0.0863    \t\tPrecision: 0.5500    \t\tRecall: 0.2515    \t\tAccuracy: 19.7400   \n",
            "\n",
            "1   \tTrain\tF1-Score=0.4142    \t\tLoss= 0.1160    \t\tPrecision: 0.5138    \t\tRecall: 0.3470    \t\tAccuracy: 19.4840   \n",
            "Validation\tF1-Score=0.3452    \t\tLoss= 0.0927    \t\tPrecision: 0.5500    \t\tRecall: 0.2515    \t\tAccuracy: 19.7400   \n",
            "\n",
            "2   \tTrain\tF1-Score=0.3875    \t\tLoss= 0.1052    \t\tPrecision: 0.5455    \t\tRecall: 0.3005    \t\tAccuracy: 19.6930   \n",
            "Validation\tF1-Score=0.4129    \t\tLoss= 0.0861    \t\tPrecision: 0.5450    \t\tRecall: 0.3323    \t\tAccuracy: 19.8000   \n",
            "\n",
            "3   \tTrain\tF1-Score=0.3815    \t\tLoss= 0.1039    \t\tPrecision: 0.5511    \t\tRecall: 0.2917    \t\tAccuracy: 19.7190   \n",
            "Validation\tF1-Score=0.4129    \t\tLoss= 0.0811    \t\tPrecision: 0.5450    \t\tRecall: 0.3323    \t\tAccuracy: 19.8000   \n",
            "\n"
          ]
        }
      ],
      "source": [
        "#torch.cuda.empty_cache()\n",
        "for i in range(2):\n",
        "    font_name = models[i]\n",
        "    print(font_name)\n",
        "\n",
        "    train_loader = train_loaders[i]\n",
        "    val_loader = val_loaders[i]\n",
        "    best_model_path = os.path.join(models_dir, f\"{font_name}_model.pt\")\n",
        "    best_results = training_supervised(\n",
        "        supervised_model,\n",
        "        train_loader,\n",
        "        val_loader,\n",
        "        criterion,\n",
        "        optimizer,\n",
        "        scheduler,\n",
        "        device,\n",
        "        epochs,\n",
        "        best_model_path,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BbquceNlLUAK"
      },
      "outputs": [],
      "source": [
        "!unzip VFR_labelled.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZY3MG-mFx0yQ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vshTAufx2Ph"
      },
      "source": [
        "Create the test_dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u67yGBzTx1_3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from torch.utils.data._utils.collate import default_collate\n",
        "\n",
        "\n",
        "class TrainingDataset(Dataset): # Modified from the SupervisedDataset class\n",
        "    def __init__(self, root_dir, labels_path, transform = None):\n",
        "        self.root_dir = root_dir\n",
        "        self.labels_path = labels_path\n",
        "        self.data = []\n",
        "        self.transform = transform\n",
        "        with open(labels_path, newline=\"\") as labels_file:\n",
        "            labels_reader = csv.reader(labels_file)\n",
        "            next(labels_reader)  # Skip the header\n",
        "            for row in labels_reader:\n",
        "                self.data.append(row)  # a list of [filename, [chars in image]]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.root_dir, self.data[idx][0])\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        if self.transform:\n",
        "            image=self.transform(image)\n",
        "\n",
        "        label_text = self.data[idx][1]\n",
        "        label_text = label_text.replace(\" \", \"\")  # Removes spaces from the label (so that all labels are one word)\n",
        "\n",
        "        # Convert label text to array of letter counts\n",
        "        label_tensor = word_to_tensor(label_text)\n",
        "\n",
        "        return image, label_tensor\n",
        "\n",
        "\n",
        "#Define a custom collate function for the dataloader (this is a workaround\n",
        "# for the test images being different sizes)\n",
        "def custom_collate(batch):\n",
        "    # Filter out None items (if your dataset returns None for some images)\n",
        "    batch = list(filter(lambda x: x is not None, batch))\n",
        "\n",
        "    # Handle the case for an empty batch\n",
        "    if len(batch) == 0:\n",
        "        return torch.Tensor()\n",
        "\n",
        "    # Separate images and labels\n",
        "    images = [item[0] for item in batch]\n",
        "    labels = [item[1] for item in batch]\n",
        "\n",
        "    # We can't stack images of different sizes, so we just keep them in a list\n",
        "    # Alternatively, you can pad images here to the same size\n",
        "    batch = (default_collate(images), default_collate(labels))\n",
        "    return batch\n",
        "\n",
        "\n",
        "\n",
        "#Define transforms for testing data\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "test_images_dir = \"/content/VFR_labelled\" # Replace with the directory of the testing images\n",
        "test_labels_path = \"/content/VFR_labelled/real_test.csv\" # Replace with the path to the test images csv label file\n",
        "test_dataset = TrainingDataset(test_images_dir, test_labels_path, test_transform)\n",
        "\n",
        "#Create dataloader for test dataset\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "#dataloader with shuffling enabled for visualization/testing, for debugging dataloader logic\n",
        "vis_loader = DataLoader(test_dataset, batch_size=64, shuffle=True, collate_fn=custom_collate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7quY-25NiFR"
      },
      "source": [
        "Test the test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NcsL3MD-NkC-"
      },
      "outputs": [],
      "source": [
        "def tensor_to_word(tensor):\n",
        "    alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    characters = [alphabet[i] for i in range(26) if tensor[i] == 1]\n",
        "\n",
        "    return ''.join(characters)\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def show_images(images, labels, num_images=4):\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    for i in range(num_images):\n",
        "        plt.subplot(1, num_images, i + 1)\n",
        "        plt.imshow(images[i].numpy().transpose(1, 2, 0))  # Convert tensor to image format\n",
        "        plt.title('Label: ' + labels[i])\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Get a random batch of images and labels\n",
        "for images, label_tensors in vis_loader:\n",
        "    text_labels = [tensor_to_word(label_tensor) for label_tensor in label_tensors]\n",
        "    show_images(images, text_labels)\n",
        "    break  # Display only the first batch\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IvDJVok60Xws"
      },
      "outputs": [],
      "source": [
        "# Testing our models\n",
        "import torch.nn as nn\n",
        "models_dir = 'models'\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "# ...\n",
        "\n",
        "for i in range(len(models)):\n",
        "    font_name = models[i]\n",
        "    print(font_name)\n",
        "    model_path = os.path.join(models_dir, f\"{font_name}_model.pt\")\n",
        "\n",
        "    # Create an instance of your model\n",
        "    model = DeepFont(num_channels=3, num_classes=26)\n",
        "\n",
        "    # Load the state dictionary into the model\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "\n",
        "    results = evaluation(model, test_loader, criterion, device, 'Test')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
