{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mc651XAM0Xwi"
      },
      "source": [
        "Data Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujWVCX920Xwm"
      },
      "source": [
        "Dataset builder- Supervised"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZY3MG-mFx0yQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1981f8ec-4ef2-4342-ef55-51cbcf6b8ed3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/\"CPSC599 Training Data\"/unsupervised.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQKmhpQjlfVZ",
        "outputId": "b5e84f5e-ea6f-4609-c9a1-297b13f43447"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/CPSC599 Training Data/unsupervised.zip\n",
            "replace unsupervised/Adding_noise2400.png? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/\"CPSC599 Training Data\"/unsupervised_synthetic.zip"
      ],
      "metadata": {
        "id": "XdxLOJXmlxv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-ocr"
      ],
      "metadata": {
        "id": "5HlBFuke5tlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Label all the data\n",
        "\n",
        "import os\n",
        "import keras_ocr\n",
        "\n",
        "pipeline = keras_ocr.pipeline.Pipeline()\n",
        "\n",
        "def process_images(image_dir, image_names):\n",
        "    images = []\n",
        "    for img in image_names:\n",
        "        img_path = os.path.join(image_dir, img)\n",
        "        image = keras_ocr.tools.read(img_path)\n",
        "        images.append(image)\n",
        "    return images\n",
        "\n",
        "batch_size = 10\n",
        "root_dir = \"/content/unsupervised\"\n",
        "total_images = os.listdir(root_dir)\n",
        "num_batches = (len(total_images) + batch_size - 1) // batch_size\n",
        "\n",
        "images_real = []\n",
        "labels_real = []\n",
        "\n",
        "for batch_idx in range(num_batches):\n",
        "    start_idx = batch_idx * batch_size\n",
        "    end_idx = (batch_idx + 1) * batch_size\n",
        "    batch_images = process_images(root_dir, total_images[start_idx:end_idx])\n",
        "\n",
        "    # Process batch_images as needed\n",
        "    batch_labels = pipeline.recognize(batch_images)\n",
        "    images_real.extend(batch_images)\n",
        "    labels_real.extend(batch_labels)\n",
        "\n",
        "    del batch_images\n",
        "    del batch_labels\n",
        "\n",
        "root_dir = \"/content/unsupervised_synthetic\"\n",
        "total_images = os.listdir(root_dir)\n",
        "num_batches = (len(total_images) + batch_size - 1) // batch_size\n",
        "\n",
        "images_synth = []\n",
        "labels_synth = []\n",
        "\n",
        "for batch_idx in range(num_batches):\n",
        "    start_idx = batch_idx * batch_size\n",
        "    end_idx = (batch_idx + 1) * batch_size\n",
        "    batch_images = process_images(root_dir, total_images[start_idx:end_idx])\n",
        "\n",
        "    # Process batch_images as needed\n",
        "    batch_labels = pipeline.recognize(batch_images)\n",
        "    images_synth.extend(batch_images)\n",
        "    labels_synth.extend(batch_labels)\n",
        "\n",
        "    del batch_images\n",
        "    del batch_labels\n",
        "\n",
        "images = images_real + images_synth\n",
        "labels = labels_real + labels_synth\n",
        "\n",
        "del images_real\n",
        "del labels_real\n",
        "del images_synth\n",
        "del labels_synth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqDU3Yn__Ytg",
        "outputId": "a857e826-ca99-4def-9846-95bc31223a13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking for /root/.keras-ocr/craft_mlt_25k.h5\n",
            "Looking for /root/.keras-ocr/crnn_kurapan.h5\n",
            "1/1 [==============================] - 7s 7s/step\n",
            "1/1 [==============================] - 3s 3s/step\n",
            "1/1 [==============================] - 12s 12s/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 7s 7s/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 29s 29s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 7s 7s/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 12s 12s/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 12s 12s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 23s 23s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 17s 17s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 24s 24s/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 11s 11s/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 16s 16s/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 29s 29s/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 16s 16s/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 9s 9s/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 10s 10s/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 19s 19s/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 13s 13s/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 11s 11s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 7s 7s/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 9s 9s/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 11s 11s/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 10s 10s/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 20s 20s/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 11s 11s/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 9s 9s/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 9s 9s/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 94ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 14s 14s/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "1/1 [==============================] - 3s 3s/step\n",
            "1/1 [==============================] - 20s 20s/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 32s 32s/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 7s 7s/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 12s 12s/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 7s 7s/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 3s 3s/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 9s 9s/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 11s 11s/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 11s 11s/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 10s 10s/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 9s 9s/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 18s 18s/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 10s 10s/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 7s 7s/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 9s 9s/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 8s 8s/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 7s 7s/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 5s 5s/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 8s 8s/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 8s 8s/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 9s 9s/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 9s 9s/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 11s 11s/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 8s 8s/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 11s 11s/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 9s 9s/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 7s 7s/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 5s 5s/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 7s 7s/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 9s 9s/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 10s 10s/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 10s 10s/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 9s 9s/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 7s 7s/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 10s 10s/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 15s 15s/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 14s 14s/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 7s 7s/step\n",
            "1/1 [==============================] - 1s 981ms/step\n",
            "1/1 [==============================] - 11s 11s/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 11s 11s/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 9s 9s/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 9s 9s/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 5s 5s/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 12s 12s/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 4s 4s/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 9s 9s/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 4s 4s/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 10s 10s/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 10s 10s/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 5s 5s/step\n",
            "1/1 [==============================] - 0s 53ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels[0][0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "9yvXp97YETqQ",
        "outputId": "90ebb8ee-981d-405c-9b78-59863df8a447"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'u'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels[1][0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "FwG_z5MwiduT",
        "outputId": "b1efba3c-b301-4e7d-94d7-eebc31b66c34"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'c'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Format labels list\n",
        "\n",
        "import csv\n",
        "\n",
        "chars_list = ['a','b','c','d','e','f','g','h','i','j','k','l','m',\n",
        "              'n','o','p','q','r','s','t','u','v','w','x','y','z']\n",
        "\n",
        "labels_list = []\n",
        "null_indices = []\n",
        "\n",
        "for i in range(len(labels)):\n",
        "  if (labels[i] == []):\n",
        "    null_indices.append(i)\n",
        "  elif (labels[i][0][0].lower() not in chars_list):\n",
        "    null_indices.append(i)\n",
        "  else:\n",
        "    labels_list.append(labels[i][0][0].lower())\n",
        "\n",
        "with open('chars_labels.csv', 'w', newline='') as csvfile:\n",
        "    csvwriter = csv.writer(csvfile, delimiter=' ',\n",
        "                            quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
        "    for item in labels_list:\n",
        "      csvwriter.writerow([item])"
      ],
      "metadata": {
        "id": "j_6WP89FfuoU"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images_list = []\n",
        "\n",
        "for i in range(len(images)):\n",
        "  if (i not in null_indices):\n",
        "    images_list.append(images[i])"
      ],
      "metadata": {
        "id": "M9eFIhVMlyr_"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into training, validation, and testing sets\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "images_train, images_test, labels_train, labels_test = train_test_split(\n",
        "    images_list, labels_list, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "images_train, images_val, labels_train, labels_val = train_test_split(\n",
        "    images_train, labels_train, test_size=0.2, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "N-GIXaqy7V4p"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "6zZVhUCht766"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kS3oxLw0Xwm",
        "outputId": "601e40e0-ec5e-4c37-a916-84ba48f735f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a: 395\n",
            "b: 298\n",
            "c: 411\n",
            "d: 446\n",
            "e: 336\n",
            "f: 310\n",
            "g: 345\n",
            "h: 369\n",
            "i: 101\n",
            "j: 165\n",
            "k: 338\n",
            "l: 337\n",
            "m: 283\n",
            "n: 414\n",
            "o: 288\n",
            "p: 339\n",
            "q: 221\n",
            "r: 287\n",
            "s: 433\n",
            "t: 431\n",
            "u: 213\n",
            "v: 207\n",
            "w: 334\n",
            "x: 247\n",
            "y: 312\n",
            "z: 253\n",
            "Class Weights: tensor([0.7900, 1.0471, 0.7592, 0.6996, 0.9287, 1.0066, 0.9045, 0.8456, 3.0895,\n",
            "        1.8911, 0.9232, 0.9259, 1.1026, 0.7537, 1.0835, 0.9205, 1.4119, 1.0872,\n",
            "        0.7206, 0.7240, 1.4650, 1.5074, 0.9342, 1.2633, 1.0001, 1.2334])\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import csv\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "class SupervisedDataset(Dataset):\n",
        "    def __init__(self, images, labels, transform = None):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        label = self.labels[idx]\n",
        "        return image, label\n",
        "\n",
        "# Define the transform\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((50,50)),\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "    transforms.RandomRotation(20),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "train_data = SupervisedDataset(images_train, labels_train, transform)\n",
        "val_data = SupervisedDataset(images_val, labels_val, transform)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=32, shuffle=False)\n",
        "\n",
        "letter_counts = {chr(ord('a') + i): 0 for i in range(26)}\n",
        "\n",
        "# Iterate through the dataset and update letter counts\n",
        "for _, label in train_data:\n",
        "    letter_counts[label] += 1\n",
        "\n",
        "# Print the counts for each letter\n",
        "for letter, count in letter_counts.items():\n",
        "    print(f\"{letter}: {count}\")\n",
        "\n",
        "class_counts = torch.tensor([letter_counts[letter] for letter in sorted(letter_counts.keys())], dtype=torch.float32)\n",
        "\n",
        "# Calculate class weights\n",
        "total_samples = len(train_data)\n",
        "class_weights = total_samples / (26 * class_counts)\n",
        "\n",
        "# Print the calculated class weights\n",
        "print(\"Class Weights:\", class_weights)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "PtnGJaRkS1L5"
      },
      "outputs": [],
      "source": [
        "class DeepFont(nn.Module):\n",
        "  def __init__(self, num_channels, num_classes):\n",
        "    super().__init__()\n",
        "\n",
        "    # self.baby = nn.Linear(in_features=256*256*num_channels, out_features=num_classes)\n",
        "\n",
        "    self.conv1 = nn.Conv2d(\n",
        "        in_channels=num_channels,\n",
        "        out_channels=64,\n",
        "        kernel_size=11,\n",
        "        padding=1,\n",
        "        stride=2\n",
        "    )\n",
        "    self.conv2 = nn.Conv2d(\n",
        "        in_channels=64,\n",
        "        out_channels=128,\n",
        "        kernel_size=5,\n",
        "        padding=2\n",
        "    )\n",
        "    self.conv3 = nn.Conv2d(\n",
        "        in_channels=128,\n",
        "        out_channels=256,\n",
        "        kernel_size=3,\n",
        "        padding=1\n",
        "    )\n",
        "    self.conv4 = nn.Conv2d(\n",
        "        in_channels=256,\n",
        "        out_channels=256,\n",
        "        kernel_size=3,\n",
        "        padding=1\n",
        "    )\n",
        "    self.conv5 = nn.Conv2d(\n",
        "        in_channels=256,\n",
        "        out_channels=256,\n",
        "        kernel_size=3,\n",
        "        padding=1\n",
        "    )\n",
        "    self.fc6 = nn.Linear(in_features=31*31*256, out_features=4096) # assuming input image size of 256x256. change in_feats for different sample size\n",
        "    self.fc7 = nn.Linear(in_features=4096, out_features=4096)\n",
        "    self.fc8 = nn.Linear(in_features=4096, out_features=num_classes)\n",
        "    self.norm1 = nn.BatchNorm2d(num_features=64)\n",
        "    self.norm2 = nn.BatchNorm2d(num_features=128)\n",
        "    self.dropout = nn.Dropout(0.5)\n",
        "    self.maxpool = nn.MaxPool2d(2)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.softmax = nn.Softmax()\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x = self.flatten(x)\n",
        "    # x = self.baby(x)\n",
        "\n",
        "    x = self.conv1(x)\n",
        "    x = self.norm1(x)\n",
        "    x = self.maxpool(x)\n",
        "    x = self.relu(x)\n",
        "\n",
        "    x = self.conv2(x)\n",
        "    x = self.norm2(x)\n",
        "    x = self.maxpool(x)\n",
        "    x = self.relu(x)\n",
        "\n",
        "    x = self.conv3(x)\n",
        "    x = self.relu(x)\n",
        "\n",
        "    x = self.conv4(x)\n",
        "    x = self.relu(x)\n",
        "\n",
        "    x = self.conv5(x)\n",
        "    x = self.relu(x)\n",
        "\n",
        "    x = self.flatten(x)\n",
        "\n",
        "    x = self.dropout(self.fc6(x))\n",
        "    x = self.relu(x)\n",
        "\n",
        "    x = self.dropout(self.fc7(x))\n",
        "    x = self.relu(x)\n",
        "\n",
        "    x = self.fc8(x)\n",
        "\n",
        "    return self.softmax(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "CyIpyIs-0Xwp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def evaluation(model, dataloader, criterion, device, phase='Validation'):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    ground_truth = []\n",
        "\n",
        "    true_positives = 0\n",
        "    true_negatives = 0\n",
        "    false_positives = 0\n",
        "    false_negatives = 0\n",
        "\n",
        "    misclassified_examples = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        total_loss = 0\n",
        "        total_samples = 0\n",
        "\n",
        "        for _, (images, labels) in enumerate(dataloader):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "            #print(loss)\n",
        "            total_loss += loss.item() * images.size(0)\n",
        "            total_samples += images.size(0)\n",
        "\n",
        "            # Update multi-class metrics\n",
        "            true_positives += (preds * labels).sum().item()\n",
        "            true_negatives += ((1 - labels) * (1 - preds)).sum().item()\n",
        "            false_positives += ((1 - labels) * preds).sum().item()\n",
        "            false_negatives += (labels * (1 - preds)).sum().item()\n",
        "\n",
        "            # Collect misclassified examples\n",
        "\n",
        "\n",
        "            predictions.extend(preds.cpu().numpy())\n",
        "            ground_truth.extend(labels.cpu().numpy())\n",
        "\n",
        "        # Calculate multi-class metrics\n",
        "        precision = true_positives / (true_positives + false_positives + 1e-10)\n",
        "        recall = true_positives / (true_positives + false_negatives + 1e-10)\n",
        "        f1_score = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
        "\n",
        "        accuracy = (true_positives + true_negatives) / (total_samples + 1e-10)\n",
        "        loss = total_loss / total_samples\n",
        "\n",
        "        print(f'{phase}\\tF1-Score={f1_score:<10.4f}' +\n",
        "              f'\\t\\tLoss= {loss:<10.4f}' +\n",
        "              f'\\t\\tPrecision: {precision:<10.4f}' +\n",
        "              f'\\t\\tRecall: {recall:<10.4f}' +\n",
        "              f'\\t\\tAccuracy: {accuracy:<10.4f}')\n",
        "\n",
        "        return {'loss': loss,\n",
        "                'f1_score': f1_score,\n",
        "                'precision': precision,\n",
        "                'recall': recall,\n",
        "                'accuracy': accuracy,\n",
        "                'ground_truth': ground_truth,\n",
        "                'predictions': predictions}\n",
        "\n",
        "# Example usage:\n",
        "# Replace 'your_model' and 'your_dataloader' with your actual model and dataloader\n",
        "# Replace 'your_device' with 'cuda' or 'cpu' depending on your setup\n",
        "# evaluation_results = evaluation(your_model, your_dataloader, criterion, your_device)\n",
        "# misclassified_examples = evaluation_results['misclassified_examples']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "Cns4YsEL0Xwq"
      },
      "outputs": [],
      "source": [
        "from torch.optim import lr_scheduler\n",
        "\n",
        "def training_supervised(model, train_loader, val_loader, criterion, optimizer, scheduler, device, epochs, best_model_path):\n",
        "    model = model.to(device)\n",
        "    model.train()\n",
        "    best_loss = torch.inf\n",
        "    best_results = None\n",
        "    # youre_on_thin_ice_buster = False\n",
        "    #misclassified_examples = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        total_samples = 0\n",
        "\n",
        "        # New variables for multi-label metrics\n",
        "        true_positives = 0\n",
        "        true_negatives = 0\n",
        "        false_positives = 0\n",
        "        false_negatives = 0\n",
        "\n",
        "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item() * images.size(0)\n",
        "            total_samples += images.size(0)\n",
        "\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            # Update multi-class metrics\n",
        "            true_positives += (preds * labels).sum().item()\n",
        "            true_negatives += ((1 - labels) * (1 - preds)).sum().item()\n",
        "            false_positives += ((1 - labels) * preds).sum().item()\n",
        "            false_negatives += (labels * (1 - preds)).sum().item()\n",
        "\n",
        "        # this is outside of batch loop\n",
        "        scheduler.step()\n",
        "\n",
        "        # Calculate multi-class metrics\n",
        "        precision = true_positives / (true_positives + false_positives + 1e-10)\n",
        "        recall = true_positives / (true_positives + false_negatives + 1e-10)\n",
        "        f1_score = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
        "\n",
        "        accuracy = (true_positives + true_negatives) / (total_samples + 1e-10)\n",
        "        loss = total_loss / total_samples\n",
        "\n",
        "        print(f'{epoch:<4}\\tTrain\\tF1-Score={f1_score:<10.4f}' +\n",
        "              f'\\t\\tLoss= {loss:<10.4f}' +\n",
        "              f'\\t\\tPrecision: {precision:<10.4f}' +\n",
        "              f'\\t\\tRecall: {recall:<10.4f}' +\n",
        "              f'\\t\\tAccuracy: {accuracy:<10.4f}')\n",
        "\n",
        "        results = evaluation(model, val_loader, criterion, device)\n",
        "        model.train()\n",
        "\n",
        "        # early stopping:\n",
        "        if results['loss'] < best_loss:             # we are still improving\n",
        "            torch.save(model.state_dict(), best_model_path)\n",
        "            best_loss = results['loss']\n",
        "            best_results = results\n",
        "        #     youre_on_thin_ice_buster = False\n",
        "        # elif youre_on_thin_ice_buster:              # we didn't improve last time and we didn't improve this time\n",
        "        #     break\n",
        "        # else:                                       # we didn't improve this time, but it was the first time in a while\n",
        "        #     youre_on_thin_ice_buster = True\n",
        "\n",
        "        #if epoch == epochs - 1:  # Check if it's the last epoch\n",
        "         # misclassified_examples = find_misclassified_examples(model, val_loader, device)\n",
        "          #print(\"Misclassified Examples:\")\n",
        "          #for example in misclassified_examples:\n",
        "           #   print(example)\n",
        "\n",
        "        for name, param in model.named_parameters():\n",
        "              if param.requires_grad and param.grad is not None:\n",
        "               # print(f\"Layer: {name}, Gradient Norm: {param.grad.norm().item()}\")\n",
        "               pass\n",
        "        print()\n",
        "\n",
        "\n",
        "    # Print misclassified examples after the last epoch\n",
        "\n",
        "    # Print misclassified examples after the last epoch\n",
        "    #print(\"Misclassified Examples:\")\n",
        "    #for example in misclassified_examples:\n",
        "     #   print(example)\n",
        "\n",
        "    return best_results\n",
        "\n",
        "def find_misclassified_examples(model, data_loader, device):\n",
        "    model.eval()\n",
        "    misclassified_examples = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in data_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            misclassified_mask = (preds != labels)\n",
        "            misclassified_indices = torch.nonzero(misclassified_mask).squeeze()\n",
        "\n",
        "            for idx in misclassified_indices:\n",
        "                # Append to misclassified examples without moving to CPU\n",
        "                misclassified_examples.append({\n",
        "                    'image': images[idx].clone(),  # Use clone to avoid modifying the original tensor\n",
        "                    'predicted_label': preds[idx].clone(),\n",
        "                    'true_label': labels[idx].clone()\n",
        "                })\n",
        "\n",
        "    return misclassified_examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qC-mt2PuRmjD",
        "outputId": "0ff3b411-9e68-4da8-e3a3-11dd4b12f0dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8113\n"
          ]
        }
      ],
      "source": [
        "print(len(train_data))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "l2trhGSP0Xwr"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "models_dir = 'models'\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "# Train the unsupervised sub-network IS NOT IN THIS NOTEBOOK ANYMORE GO SEE OTHER NOTEBOOK\n",
        "\n",
        "\n",
        "# Train the supervised sub-network\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.01 # we don't have all day\n",
        "momentum = 0.95\n",
        "weight_decay = 1e-4\n",
        "epochs = 16 # CHANGED BY XANNA\n",
        "criterion = torch.nn.CrossEntropyLoss()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "QRb9Co1mVQ2S"
      },
      "outputs": [],
      "source": [
        "# Stops colab from breaking sometimes\n",
        "# Only works sometimes\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgk8Xxqb0Xwr"
      },
      "outputs": [],
      "source": [
        "# define supervised model\n",
        "supervised_model = DeepFont(\n",
        "    num_channels=3, num_classes=26\n",
        ")  # one class per letter (not case-sensitive)\n",
        "\n",
        "# Not using the unsupervised sub-network since we're just using the Chars74 dataset\n",
        "# Import the convolutional layers of the SCAE as conv1 and conv2\n",
        "# scae_path = os.path.join(models_dir, f\"SCAE.pt\")\n",
        "# supervised_model.load_state_dict(torch.load(scae_path), strict=False)\n",
        "\n",
        "# Freeze the convolutional layers from SCAE\n",
        "for param in supervised_model.conv1.parameters():\n",
        "    param.requires_grad = False\n",
        "for param in supervised_model.conv2.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# unfreeze layers                                   # something something when i removed coconunt.jpg the whole project broke and we don;t even wanna know if this is a coconut\n",
        "for param in supervised_model.conv3.parameters():\n",
        "    param.requires_grad = True\n",
        "for param in supervised_model.conv4.parameters():\n",
        "    param.requires_grad = True\n",
        "for param in supervised_model.conv5.parameters():\n",
        "    param.requires_grad = True\n",
        "for param in supervised_model.fc6.parameters():\n",
        "    param.requires_grad = True\n",
        "for param in supervised_model.fc7.parameters():\n",
        "    param.requires_grad = True\n",
        "for param in supervised_model.fc8.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHc02LvhVQ2T"
      },
      "outputs": [],
      "source": [
        "# define optimizer and scheduler :) thank u xanna. ur welcome\n",
        "optimizer = optim.SGD(supervised_model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-JdELVNVQ2T"
      },
      "outputs": [],
      "source": [
        "#torch.cuda.empty_cache()\n",
        "\n",
        "# TODO: CHANGE THE i VALUE BEFORE YOU TRAIN (xanna 0 and 1; seher 2 and 3; quyanna 4 and 5; allison 6)\n",
        "# AND ALSO RERUN THE PREVIOUS 2 CELLS (REDEFINE THE SUPERVISED MODEL)\n",
        "best_model_path = os.path.join(models_dir, f\"char_classifier.pt\")\n",
        "best_results = training_supervised(\n",
        "    supervised_model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    criterion,\n",
        "    optimizer,\n",
        "    scheduler,\n",
        "    device,\n",
        "    epochs,\n",
        "    best_model_path,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BbquceNlLUAK"
      },
      "outputs": [],
      "source": [
        "!unzip VFR_labelled.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vshTAufx2Ph"
      },
      "source": [
        "Create the test_dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u67yGBzTx1_3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from torch.utils.data._utils.collate import default_collate\n",
        "\n",
        "\n",
        "class TrainingDataset(Dataset): # Modified from the SupervisedDataset class\n",
        "    def __init__(self, root_dir, labels_path, transform = None):\n",
        "        self.root_dir = root_dir\n",
        "        self.labels_path = labels_path\n",
        "        self.data = []\n",
        "        self.transform = transform\n",
        "        with open(labels_path, newline=\"\") as labels_file:\n",
        "            labels_reader = csv.reader(labels_file)\n",
        "            next(labels_reader)  # Skip the header\n",
        "            for row in labels_reader:\n",
        "                self.data.append(row)  # a list of [filename, [chars in image]]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.root_dir, self.data[idx][0])\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        if self.transform:\n",
        "            image=self.transform(image)\n",
        "\n",
        "        label_text = self.data[idx][1]\n",
        "        label_text = label_text.replace(\" \", \"\")  # Removes spaces from the label (so that all labels are one word)\n",
        "\n",
        "        return image, label_tensor\n",
        "\n",
        "\n",
        "#Define a custom collate function for the dataloader (this is a workaround\n",
        "# for the test images being different sizes)\n",
        "def custom_collate(batch):\n",
        "    # Filter out None items (if your dataset returns None for some images)\n",
        "    batch = list(filter(lambda x: x is not None, batch))\n",
        "\n",
        "    # Handle the case for an empty batch\n",
        "    if len(batch) == 0:\n",
        "        return torch.Tensor()\n",
        "\n",
        "    # Separate images and labels\n",
        "    images = [item[0] for item in batch]\n",
        "    labels = [item[1] for item in batch]\n",
        "\n",
        "    # We can't stack images of different sizes, so we just keep them in a list\n",
        "    # Alternatively, you can pad images here to the same size\n",
        "    batch = (default_collate(images), default_collate(labels))\n",
        "    return batch\n",
        "\n",
        "\n",
        "\n",
        "#Define transforms for testing data\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "test_images_dir = \"/content/VFR_labelled\" # Replace with the directory of the testing images\n",
        "test_labels_path = \"/content/VFR_labelled/real_test.csv\" # Replace with the path to the test images csv label file\n",
        "test_dataset = TrainingDataset(test_images_dir, test_labels_path, test_transform)\n",
        "\n",
        "#Create dataloader for test dataset\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "#dataloader with shuffling enabled for visualization/testing, for debugging dataloader logic\n",
        "vis_loader = DataLoader(test_dataset, batch_size=64, shuffle=True, collate_fn=custom_collate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7quY-25NiFR"
      },
      "source": [
        "Test the test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NcsL3MD-NkC-"
      },
      "outputs": [],
      "source": [
        "def tensor_to_word(tensor):\n",
        "    alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    characters = [alphabet[i] for i in range(26) if tensor[i] == 1]\n",
        "\n",
        "    return ''.join(characters)\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def show_images(images, labels, num_images=4):\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    for i in range(num_images):\n",
        "        plt.subplot(1, num_images, i + 1)\n",
        "        plt.imshow(images[i].numpy().transpose(1, 2, 0))  # Convert tensor to image format\n",
        "        plt.title('Label: ' + labels[i])\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Get a random batch of images and labels\n",
        "for images, label_tensors in vis_loader:\n",
        "    text_labels = [tensor_to_word(label_tensor) for label_tensor in label_tensors]\n",
        "    show_images(images, text_labels)\n",
        "    break  # Display only the first batch\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IvDJVok60Xws"
      },
      "outputs": [],
      "source": [
        "# Testing our models\n",
        "import torch.nn as nn\n",
        "models_dir = 'models'\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "# ...\n",
        "\n",
        "model_path = os.path.join(models_dir, f\"char_classifier.pt\")\n",
        "\n",
        "# Create an instance of your model\n",
        "model = DeepFont(num_channels=3, num_classes=26)\n",
        "\n",
        "# Load the state dictionary into the model\n",
        "model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "\n",
        "results = evaluation(model, test_loader, criterion, device, 'Test')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}