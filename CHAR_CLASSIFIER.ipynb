{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mc651XAM0Xwi"
      },
      "source": [
        "Data Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujWVCX920Xwm"
      },
      "source": [
        "Dataset builder- Supervised"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZY3MG-mFx0yQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "2e7b296e-7e5d-4834-b40c-85767110c839"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/\"CPSC599 Training Data\"/unsupervised.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQKmhpQjlfVZ",
        "outputId": "b5e84f5e-ea6f-4609-c9a1-297b13f43447"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/CPSC599 Training Data/unsupervised.zip\n",
            "replace unsupervised/Adding_noise2400.png? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/\"CPSC599 Training Data\"/unsupervised_synthetic.zip"
      ],
      "metadata": {
        "id": "XdxLOJXmlxv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-ocr"
      ],
      "metadata": {
        "id": "5HlBFuke5tlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Label all the data\n",
        "\n",
        "import os\n",
        "import keras_ocr\n",
        "\n",
        "pipeline = keras_ocr.pipeline.Pipeline()\n",
        "\n",
        "def process_images(image_dir, image_names):\n",
        "    images = []\n",
        "    for img in image_names:\n",
        "        img_path = os.path.join(image_dir, img)\n",
        "        image = keras_ocr.tools.read(img_path)\n",
        "        images.append(image)\n",
        "    return images\n",
        "\n",
        "batch_size = 10\n",
        "root_dir = \"/content/unsupervised\"\n",
        "total_images = os.listdir(root_dir)\n",
        "num_batches = (len(total_images) + batch_size - 1) // batch_size\n",
        "\n",
        "images_real = []\n",
        "labels_real = []\n",
        "\n",
        "for batch_idx in range(num_batches):\n",
        "    start_idx = batch_idx * batch_size\n",
        "    end_idx = (batch_idx + 1) * batch_size\n",
        "    batch_images = process_images(root_dir, total_images[start_idx:end_idx])\n",
        "\n",
        "    # Process batch_images as needed\n",
        "    batch_labels = pipeline.recognize(batch_images)\n",
        "    images_real.extend(batch_images)\n",
        "    labels_real.extend(batch_labels)\n",
        "\n",
        "    del batch_images\n",
        "    del batch_labels\n",
        "\n",
        "root_dir = \"/content/unsupervised_synthetic\"\n",
        "total_images = os.listdir(root_dir)\n",
        "num_batches = (len(total_images) + batch_size - 1) // batch_size\n",
        "\n",
        "images_synth = []\n",
        "labels_synth = []\n",
        "\n",
        "for batch_idx in range(num_batches):\n",
        "    start_idx = batch_idx * batch_size\n",
        "    end_idx = (batch_idx + 1) * batch_size\n",
        "    batch_images = process_images(root_dir, total_images[start_idx:end_idx])\n",
        "\n",
        "    # Process batch_images as needed\n",
        "    batch_labels = pipeline.recognize(batch_images)\n",
        "    images_synth.extend(batch_images)\n",
        "    labels_synth.extend(batch_labels)\n",
        "\n",
        "    del batch_images\n",
        "    del batch_labels\n",
        "\n",
        "images = images_real + images_synth\n",
        "labels = labels_real + labels_synth\n",
        "\n",
        "del images_real\n",
        "del labels_real\n",
        "del images_synth\n",
        "del labels_synth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqDU3Yn__Ytg",
        "outputId": "3b6a7953-1986-44f4-a5f2-1bc441170df5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking for /root/.keras-ocr/craft_mlt_25k.h5\n",
            "Downloading /root/.keras-ocr/craft_mlt_25k.h5\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1260: resize_bilinear (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.image.resize(...method=ResizeMethod.BILINEAR...)` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking for /root/.keras-ocr/crnn_kurapan.h5\n",
            "1/1 [==============================] - 7s 7s/step\n",
            "1/1 [==============================] - 3s 3s/step\n",
            "1/1 [==============================] - 12s 12s/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 7s 7s/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 29s 29s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 7s 7s/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 12s 12s/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 12s 12s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 23s 23s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 17s 17s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 24s 24s/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 11s 11s/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 16s 16s/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 29s 29s/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 16s 16s/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 9s 9s/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 10s 10s/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 19s 19s/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 13s 13s/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 11s 11s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 7s 7s/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 9s 9s/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 11s 11s/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 10s 10s/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 20s 20s/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 11s 11s/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 9s 9s/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 9s 9s/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 94ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 14s 14s/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "1/1 [==============================] - 3s 3s/step\n",
            "1/1 [==============================] - 20s 20s/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 32s 32s/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 7s 7s/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 12s 12s/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 7s 7s/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 3s 3s/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 9s 9s/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 11s 11s/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 11s 11s/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 10s 10s/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 9s 9s/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 18s 18s/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 10s 10s/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 7s 7s/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 9s 9s/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 8s 8s/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 7s 7s/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 5s 5s/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 8s 8s/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 8s 8s/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 9s 9s/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 9s 9s/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 11s 11s/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 8s 8s/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 11s 11s/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 9s 9s/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 7s 7s/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 5s 5s/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 7s 7s/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 9s 9s/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 10s 10s/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 10s 10s/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 9s 9s/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 7s 7s/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 10s 10s/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 15s 15s/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 14s 14s/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 7s 7s/step\n",
            "1/1 [==============================] - 1s 981ms/step\n",
            "1/1 [==============================] - 11s 11s/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 11s 11s/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 9s 9s/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 9s 9s/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 5s 5s/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 12s 12s/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 4s 4s/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 9s 9s/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 4s 4s/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 10s 10s/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 10s 10s/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 5s 5s/step\n",
            "1/1 [==============================] - 0s 53ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels[0][0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "9yvXp97YETqQ",
        "outputId": "dcbd5dae-6111-421d-d4fd-c13cee18ab41"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'u'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Format labels list\n",
        "\n",
        "import csv\n",
        "\n",
        "chars_list = ['a','b','c','d','e','f','g','h','i','j','k','l','m',\n",
        "              'n','o','p','q','r','s','t','u','v','w','x','y','z']\n",
        "\n",
        "labels_list = []\n",
        "null_indices = []\n",
        "\n",
        "for i in range(len(labels)):\n",
        "  if (labels[i] == []):\n",
        "    null_indices.append(i)\n",
        "  elif (labels[i][0][0].lower() not in chars_list):\n",
        "    null_indices.append(i)\n",
        "  else:\n",
        "    labels_list.append(labels[i][0][0].lower())\n",
        "\n",
        "with open('chars_labels.csv', 'w', newline='') as csvfile:\n",
        "    csvwriter = csv.writer(csvfile, delimiter=' ',\n",
        "                            quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
        "    for item in labels_list:\n",
        "      csvwriter.writerow([item])\n",
        "\n",
        "with open('null_indices.csv', 'w', newline='') as csvfile:\n",
        "    csvwriter = csv.writer(csvfile, delimiter=' ',\n",
        "                            quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
        "    for item in null_indices:\n",
        "      csvwriter.writerow([item])"
      ],
      "metadata": {
        "id": "j_6WP89FfuoU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "le.fit(chars_list)\n",
        "\n",
        "labels_list = le.transform(labels_list)"
      ],
      "metadata": {
        "id": "mBCdE_qPmaMN"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images_list = []\n",
        "\n",
        "for i in range(len(images)):\n",
        "  if (i not in null_indices):\n",
        "    images_list.append(images[i])"
      ],
      "metadata": {
        "id": "M9eFIhVMlyr_"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into training, validation, and testing sets\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "images_train, images_test, labels_train, labels_test = train_test_split(\n",
        "    images_list, labels_list, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "images_train, images_val, labels_train, labels_val = train_test_split(\n",
        "    images_train, labels_train, test_size=0.2, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "N-GIXaqy7V4p"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "6zZVhUCht766"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kS3oxLw0Xwm",
        "outputId": "4313765f-ddb9-41b1-a28f-ddef66dcb672"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a: 395\n",
            "b: 298\n",
            "c: 411\n",
            "d: 446\n",
            "e: 336\n",
            "f: 310\n",
            "g: 345\n",
            "h: 369\n",
            "i: 101\n",
            "j: 165\n",
            "k: 338\n",
            "l: 337\n",
            "m: 283\n",
            "n: 414\n",
            "o: 288\n",
            "p: 339\n",
            "q: 221\n",
            "r: 287\n",
            "s: 433\n",
            "t: 431\n",
            "u: 213\n",
            "v: 207\n",
            "w: 334\n",
            "x: 247\n",
            "y: 312\n",
            "z: 253\n",
            "Class Weights: tensor([0.7900, 1.0471, 0.7592, 0.6996, 0.9287, 1.0066, 0.9045, 0.8456, 3.0895,\n",
            "        1.8911, 0.9232, 0.9259, 1.1026, 0.7537, 1.0835, 0.9205, 1.4119, 1.0872,\n",
            "        0.7206, 0.7240, 1.4650, 1.5074, 0.9342, 1.2633, 1.0001, 1.2334])\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import csv\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "class SupervisedDataset(Dataset):\n",
        "    def __init__(self, images, labels, transform = None):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        label = self.labels[idx]\n",
        "        return image, label\n",
        "\n",
        "# Define the transform\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((256,256)),\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "    transforms.RandomRotation(20),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "train_data = SupervisedDataset(images_train, labels_train, transform)\n",
        "val_data = SupervisedDataset(images_val, labels_val, transform)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=32, shuffle=False)\n",
        "\n",
        "letter_counts = {chr(ord('a') + i): 0 for i in range(26)}\n",
        "\n",
        "# Iterate through the dataset and update letter counts\n",
        "for _, label in train_data:\n",
        "    letter_counts[le.inverse_transform([label])[0]] += 1\n",
        "\n",
        "# Print the counts for each letter\n",
        "for letter, count in letter_counts.items():\n",
        "    print(f\"{letter}: {count}\")\n",
        "\n",
        "class_counts = torch.tensor([letter_counts[letter] for letter in sorted(letter_counts.keys())], dtype=torch.float32)\n",
        "\n",
        "# Calculate class weights\n",
        "total_samples = len(train_data)\n",
        "class_weights = total_samples / (26 * class_counts)\n",
        "\n",
        "# Print the calculated class weights\n",
        "print(\"Class Weights:\", class_weights)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "PtnGJaRkS1L5"
      },
      "outputs": [],
      "source": [
        "class DeepFont(nn.Module):\n",
        "  def __init__(self, num_channels, num_classes):\n",
        "    super().__init__()\n",
        "\n",
        "    # self.baby = nn.Linear(in_features=256*256*num_channels, out_features=num_classes)\n",
        "\n",
        "    self.conv1 = nn.Conv2d(\n",
        "        in_channels=num_channels,\n",
        "        out_channels=64,\n",
        "        kernel_size=11,\n",
        "        padding=1,\n",
        "        stride=2\n",
        "    )\n",
        "    self.conv2 = nn.Conv2d(\n",
        "        in_channels=64,\n",
        "        out_channels=128,\n",
        "        kernel_size=5,\n",
        "        padding=2\n",
        "    )\n",
        "    self.conv3 = nn.Conv2d(\n",
        "        in_channels=128,\n",
        "        out_channels=256,\n",
        "        kernel_size=3,\n",
        "        padding=1\n",
        "    )\n",
        "    self.conv4 = nn.Conv2d(\n",
        "        in_channels=256,\n",
        "        out_channels=256,\n",
        "        kernel_size=3,\n",
        "        padding=1\n",
        "    )\n",
        "    self.conv5 = nn.Conv2d(\n",
        "        in_channels=256,\n",
        "        out_channels=256,\n",
        "        kernel_size=3,\n",
        "        padding=1\n",
        "    )\n",
        "    self.fc6 = nn.Linear(in_features=31*31*256, out_features=4096) # assuming input image size of 256x256. change in_feats for different sample size\n",
        "    self.fc7 = nn.Linear(in_features=4096, out_features=4096)\n",
        "    self.fc8 = nn.Linear(in_features=4096, out_features=num_classes)\n",
        "    self.norm1 = nn.BatchNorm2d(num_features=64)\n",
        "    self.norm2 = nn.BatchNorm2d(num_features=128)\n",
        "    self.dropout = nn.Dropout(0.5)\n",
        "    self.maxpool = nn.MaxPool2d(2)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.softmax = nn.Softmax()\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x = self.flatten(x)\n",
        "    # x = self.baby(x)\n",
        "\n",
        "    x = self.conv1(x)\n",
        "    x = self.norm1(x)\n",
        "    x = self.maxpool(x)\n",
        "    x = self.relu(x)\n",
        "\n",
        "    x = self.conv2(x)\n",
        "    x = self.norm2(x)\n",
        "    x = self.maxpool(x)\n",
        "    x = self.relu(x)\n",
        "\n",
        "    x = self.conv3(x)\n",
        "    x = self.relu(x)\n",
        "\n",
        "    x = self.conv4(x)\n",
        "    x = self.relu(x)\n",
        "\n",
        "    x = self.conv5(x)\n",
        "    x = self.relu(x)\n",
        "\n",
        "    x = self.flatten(x)\n",
        "\n",
        "    x = self.dropout(self.fc6(x))\n",
        "    x = self.relu(x)\n",
        "\n",
        "    x = self.dropout(self.fc7(x))\n",
        "    x = self.relu(x)\n",
        "\n",
        "    x = self.fc8(x)\n",
        "\n",
        "    return self.softmax(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "CyIpyIs-0Xwp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def evaluation(model, dataloader, criterion, device, phase='Validation'):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    ground_truth = []\n",
        "\n",
        "    true_positives = 0\n",
        "    true_negatives = 0\n",
        "    false_positives = 0\n",
        "    false_negatives = 0\n",
        "\n",
        "    misclassified_examples = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        total_loss = 0\n",
        "        total_samples = 0\n",
        "\n",
        "        for _, (images, labels) in enumerate(dataloader):\n",
        "            images = images.to(device)\n",
        "            labels_tensor = torch.tensor([label for label in labels])\n",
        "            labels = labels_tensor.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "            #print(loss)\n",
        "            total_loss += loss.item() * images.size(0)\n",
        "            total_samples += images.size(0)\n",
        "\n",
        "            # Update multi-class metrics\n",
        "            true_positives += (preds * labels).sum().item()\n",
        "            true_negatives += ((1 - labels) * (1 - preds)).sum().item()\n",
        "            false_positives += ((1 - labels) * preds).sum().item()\n",
        "            false_negatives += (labels * (1 - preds)).sum().item()\n",
        "\n",
        "            # Collect misclassified examples\n",
        "\n",
        "\n",
        "            predictions.extend(preds.cpu().numpy())\n",
        "            ground_truth.extend(labels.cpu().numpy())\n",
        "\n",
        "        # Calculate multi-class metrics\n",
        "        precision = true_positives / (true_positives + false_positives + 1e-10)\n",
        "        recall = true_positives / (true_positives + false_negatives + 1e-10)\n",
        "        f1_score = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
        "\n",
        "        accuracy = (true_positives + true_negatives) / (total_samples + 1e-10)\n",
        "        loss = total_loss / total_samples\n",
        "\n",
        "        print(f'{phase}\\tF1-Score={f1_score:<10.4f}' +\n",
        "              f'\\t\\tLoss= {loss:<10.4f}' +\n",
        "              f'\\t\\tPrecision: {precision:<10.4f}' +\n",
        "              f'\\t\\tRecall: {recall:<10.4f}' +\n",
        "              f'\\t\\tAccuracy: {accuracy:<10.4f}')\n",
        "\n",
        "        return {'loss': loss,\n",
        "                'f1_score': f1_score,\n",
        "                'precision': precision,\n",
        "                'recall': recall,\n",
        "                'accuracy': accuracy,\n",
        "                'ground_truth': ground_truth,\n",
        "                'predictions': predictions}\n",
        "\n",
        "# Example usage:\n",
        "# Replace 'your_model' and 'your_dataloader' with your actual model and dataloader\n",
        "# Replace 'your_device' with 'cuda' or 'cpu' depending on your setup\n",
        "# evaluation_results = evaluation(your_model, your_dataloader, criterion, your_device)\n",
        "# misclassified_examples = evaluation_results['misclassified_examples']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "Cns4YsEL0Xwq"
      },
      "outputs": [],
      "source": [
        "from torch.optim import lr_scheduler\n",
        "\n",
        "def training_supervised(model, train_loader, val_loader, criterion, optimizer, scheduler, device, epochs, best_model_path):\n",
        "    model = model.to(device)\n",
        "    model.train()\n",
        "    best_loss = torch.inf\n",
        "    best_results = None\n",
        "    # youre_on_thin_ice_buster = False\n",
        "    #misclassified_examples = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        total_samples = 0\n",
        "\n",
        "        # New variables for multi-label metrics\n",
        "        true_positives = 0\n",
        "        true_negatives = 0\n",
        "        false_positives = 0\n",
        "        false_negatives = 0\n",
        "\n",
        "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            images = images.to(device)\n",
        "            labels_tensor = torch.tensor([label for label in labels])\n",
        "            labels = labels_tensor.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item() * images.size(0)\n",
        "            total_samples += images.size(0)\n",
        "\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            # Update multi-class metrics\n",
        "            true_positives += (preds * labels).sum().item()\n",
        "            true_negatives += ((1 - labels) * (1 - preds)).sum().item()\n",
        "            false_positives += ((1 - labels) * preds).sum().item()\n",
        "            false_negatives += (labels * (1 - preds)).sum().item()\n",
        "\n",
        "        # this is outside of batch loop\n",
        "        scheduler.step()\n",
        "\n",
        "        # Calculate multi-class metrics\n",
        "        precision = true_positives / (true_positives + false_positives + 1e-10)\n",
        "        recall = true_positives / (true_positives + false_negatives + 1e-10)\n",
        "        f1_score = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
        "\n",
        "        accuracy = (true_positives + true_negatives) / (total_samples + 1e-10)\n",
        "        loss = total_loss / total_samples\n",
        "\n",
        "        print(f'{epoch:<4}\\tTrain\\tF1-Score={f1_score:<10.4f}' +\n",
        "              f'\\t\\tLoss= {loss:<10.4f}' +\n",
        "              f'\\t\\tPrecision: {precision:<10.4f}' +\n",
        "              f'\\t\\tRecall: {recall:<10.4f}' +\n",
        "              f'\\t\\tAccuracy: {accuracy:<10.4f}')\n",
        "\n",
        "        results = evaluation(model, val_loader, criterion, device)\n",
        "        model.train()\n",
        "\n",
        "        # early stopping:\n",
        "        if results['loss'] < best_loss:             # we are still improving\n",
        "            torch.save(model.state_dict(), best_model_path)\n",
        "            best_loss = results['loss']\n",
        "            best_results = results\n",
        "        #     youre_on_thin_ice_buster = False\n",
        "        # elif youre_on_thin_ice_buster:              # we didn't improve last time and we didn't improve this time\n",
        "        #     break\n",
        "        # else:                                       # we didn't improve this time, but it was the first time in a while\n",
        "        #     youre_on_thin_ice_buster = True\n",
        "\n",
        "        #if epoch == epochs - 1:  # Check if it's the last epoch\n",
        "         # misclassified_examples = find_misclassified_examples(model, val_loader, device)\n",
        "          #print(\"Misclassified Examples:\")\n",
        "          #for example in misclassified_examples:\n",
        "           #   print(example)\n",
        "\n",
        "        for name, param in model.named_parameters():\n",
        "              if param.requires_grad and param.grad is not None:\n",
        "               # print(f\"Layer: {name}, Gradient Norm: {param.grad.norm().item()}\")\n",
        "               pass\n",
        "        print()\n",
        "\n",
        "\n",
        "    # Print misclassified examples after the last epoch\n",
        "\n",
        "    # Print misclassified examples after the last epoch\n",
        "    #print(\"Misclassified Examples:\")\n",
        "    #for example in misclassified_examples:\n",
        "     #   print(example)\n",
        "\n",
        "    return best_results\n",
        "\n",
        "def find_misclassified_examples(model, data_loader, device):\n",
        "    model.eval()\n",
        "    misclassified_examples = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in data_loader:\n",
        "            images = images.to(device)\n",
        "            labels_tensor = torch.tensor([label for label in labels])\n",
        "            labels = labels_tensor.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            misclassified_mask = (preds != labels)\n",
        "            misclassified_indices = torch.nonzero(misclassified_mask).squeeze()\n",
        "\n",
        "            for idx in misclassified_indices:\n",
        "                # Append to misclassified examples without moving to CPU\n",
        "                misclassified_examples.append({\n",
        "                    'image': images[idx].clone(),  # Use clone to avoid modifying the original tensor\n",
        "                    'predicted_label': preds[idx].clone(),\n",
        "                    'true_label': labels[idx].clone()\n",
        "                })\n",
        "\n",
        "    return misclassified_examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qC-mt2PuRmjD",
        "outputId": "31de5652-a639-43cd-b4a3-9b97b48e8bed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8113\n"
          ]
        }
      ],
      "source": [
        "print(len(train_data))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "l2trhGSP0Xwr"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "# Train the unsupervised sub-network IS NOT IN THIS NOTEBOOK ANYMORE GO SEE OTHER NOTEBOOK\n",
        "\n",
        "\n",
        "# Train the supervised sub-network\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.01 # we don't have all day\n",
        "momentum = 0.95\n",
        "weight_decay = 1e-4\n",
        "epochs = 16 # CHANGED BY XANNA\n",
        "criterion = torch.nn.CrossEntropyLoss()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "QRb9Co1mVQ2S"
      },
      "outputs": [],
      "source": [
        "# Stops colab from breaking sometimes\n",
        "# Only works sometimes\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "rgk8Xxqb0Xwr"
      },
      "outputs": [],
      "source": [
        "# define supervised model\n",
        "supervised_model = DeepFont(\n",
        "    num_channels=3, num_classes=26\n",
        ")  # one class per letter (not case-sensitive)\n",
        "\n",
        "# Freeze the convolutional layers from SCAE\n",
        "for param in supervised_model.conv1.parameters():\n",
        "    param.requires_grad = False\n",
        "for param in supervised_model.conv2.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# unfreeze layers                                   # something something when i removed coconunt.jpg the whole project broke and we don;t even wanna know if this is a coconut\n",
        "for param in supervised_model.conv3.parameters():\n",
        "    param.requires_grad = True\n",
        "for param in supervised_model.conv4.parameters():\n",
        "    param.requires_grad = True\n",
        "for param in supervised_model.conv5.parameters():\n",
        "    param.requires_grad = True\n",
        "for param in supervised_model.fc6.parameters():\n",
        "    param.requires_grad = True\n",
        "for param in supervised_model.fc7.parameters():\n",
        "    param.requires_grad = True\n",
        "for param in supervised_model.fc8.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "XHc02LvhVQ2T"
      },
      "outputs": [],
      "source": [
        "# define optimizer and scheduler :) thank u xanna. ur welcome\n",
        "optimizer = optim.SGD(supervised_model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "p-JdELVNVQ2T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "outputId": "899cf467-5a30-4636-e8f2-555c687017c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0   \tTrain\tF1-Score=8.7182    \t\tLoss= 3.2540    \t\tPrecision: 11.9939   \t\tRecall: 6.8480    \t\tAccuracy: 146.1574  \n",
            "Validation\tF1-Score=3.4166    \t\tLoss= 3.2540    \t\tPrecision: 11.7117   \t\tRecall: 2.0000    \t\tAccuracy: 34.1350   \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-81-976c8b486772>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# AND ALSO RERUN THE PREVIOUS 2 CELLS (REDEFINE THE SUPERVISED MODEL)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbest_model_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"char_classifier.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m best_results = training_supervised(\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0msupervised_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-73-335605778372>\u001b[0m in \u001b[0;36mtraining_supervised\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, scheduler, device, epochs, best_model_path)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;31m# early stopping:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_loss\u001b[0m\u001b[0;34m:\u001b[0m             \u001b[0;31m# we are still improving\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_model_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0mbest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mbest_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m             \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_disable_byteorder_record\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    490\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m         \u001b[0mcontainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_zipfile_writer_buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcontainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    461\u001b[0m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPyTorchFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_stream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPyTorchFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Parent directory /content/models does not exist."
          ]
        }
      ],
      "source": [
        "#torch.cuda.empty_cache()\n",
        "\n",
        "# TODO: CHANGE THE i VALUE BEFORE YOU TRAIN (xanna 0 and 1; seher 2 and 3; quyanna 4 and 5; allison 6)\n",
        "# AND ALSO RERUN THE PREVIOUS 2 CELLS (REDEFINE THE SUPERVISED MODEL)\n",
        "best_model_path = \"char_classifier.pt\"\n",
        "best_results = training_supervised(\n",
        "    supervised_model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    criterion,\n",
        "    optimizer,\n",
        "    scheduler,\n",
        "    device,\n",
        "    epochs,\n",
        "    best_model_path,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vshTAufx2Ph"
      },
      "source": [
        "Create the test_dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u67yGBzTx1_3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "#Define transforms for testing data\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "test_dataset = SupervisedDataset(images_test, labels_test, test_transform)\n",
        "\n",
        "#Create dataloader for test dataset\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "#dataloader with shuffling enabled for visualization/testing, for debugging dataloader logic\n",
        "vis_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7quY-25NiFR"
      },
      "source": [
        "Test the test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NcsL3MD-NkC-"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def show_images(images, labels, num_images=4):\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    for i in range(num_images):\n",
        "        plt.subplot(1, num_images, i + 1)\n",
        "        plt.imshow(images[i].numpy().transpose(1, 2, 0))  # Convert tensor to image format\n",
        "        plt.title('Label: ' + labels[i])\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Get a random batch of images and labels\n",
        "for images, labels in vis_loader:\n",
        "    text_labels = [le.inverse_transform([label])[0] for label in labels]\n",
        "    show_images(images, text_labels)\n",
        "    break  # Display only the first batch\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IvDJVok60Xws"
      },
      "outputs": [],
      "source": [
        "# Testing our models\n",
        "import torch.nn as nn\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "# ...\n",
        "\n",
        "model_path = \"char_classifier.pt\"\n",
        "\n",
        "# Create an instance of your model\n",
        "model = DeepFont(num_channels=3, num_classes=26)\n",
        "\n",
        "# Load the state dictionary into the model\n",
        "model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "\n",
        "results = evaluation(model, test_loader, criterion, device, 'Test')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}