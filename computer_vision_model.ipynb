{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Google Drive Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gdown\n",
        "import shutil\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Google Drive folder ID for your supervised and unsupervised datasets\n",
        "unsupervised_synthetic_folder_id = '18OaY3Z-bB1VAe3Z-QcB8z-k3fRlVdqsA'\n",
        "unsupervised_folder_id = '1iD4zSfL1ojSqng_jstunWYvIpqp3bZiH'\n",
        "supervised_folder_id= '1BvllRinXBvB0xAjFL_HlPshCs2X06Fn8'\n",
        "\n",
        "# Destination directories for storing the downloaded data\n",
        "supervised_dest_dir = 'path/to/your/destination/directory/supervised'\n",
        "unsupervised_dest_dir = 'path/to/your/destination/directory/unsupervised'\n",
        "\n",
        "# Create destination directories if they don't exist\n",
        "os.makedirs(supervised_dest_dir, exist_ok=True)\n",
        "os.makedirs(unsupervised_dest_dir, exist_ok=True)\n",
        "\n",
        "# Function to download a folder from Google Drive\n",
        "def download_folder(folder_id, destination):\n",
        "    url = f'https://drive.google.com/uc?id={folder_id}'\n",
        "    output = os.path.join(destination, 'temp.zip')\n",
        "    gdown.download(url, output=output, quiet=False)\n",
        "\n",
        "    with zipfile.ZipFile(output, 'r') as zip_ref:\n",
        "        zip_ref.extractall(destination)\n",
        "\n",
        "    # Move the contents of the extracted folder to the destination\n",
        "    extracted_folder = os.path.join(destination, Path(output).stem)\n",
        "    for item in os.listdir(extracted_folder):\n",
        "        s = os.path.join(extracted_folder, item)\n",
        "        d = os.path.join(destination, item)\n",
        "        if os.path.isdir(s):\n",
        "            shutil.move(s, d)\n",
        "        else:\n",
        "            shutil.copy2(s, d)\n",
        "\n",
        "    # Clean up temporary files\n",
        "    os.remove(output)\n",
        "    shutil.rmtree(extracted_folder)\n",
        "\n",
        "# Download and organize the supervised dataset\n",
        "download_folder(supervised_folder_id, supervised_dest_dir)\n",
        "\n",
        "# Download and organize the unsupervised dataset\n",
        "download_folder(unsupervised_folder_id, unsupervised_dest_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6zZVhUCht766"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4lJFJUkovEba"
      },
      "outputs": [],
      "source": [
        "# Stacked Convolutional Auto-Encoder (the unsuperfvised sub-netwerork)\n",
        "class SCAE(nn.Module):\n",
        "  def __init__(self, num_channels, num_classes):\n",
        "    super.__init__()\n",
        "\n",
        "    self.conv1 = nn.Conv2d(\n",
        "        in_channels=num_channels,\n",
        "        out_channels=64,\n",
        "        kernel_size=11,\n",
        "        padding=1,\n",
        "        stride=2\n",
        "    )\n",
        "    self.conv2 = nn.Conv2d(\n",
        "        in_channels=64,\n",
        "        out_channels=128,\n",
        "        kernel_size=5,\n",
        "        padding=2\n",
        "    )\n",
        "    self.deconv1 = nn.ConvTranspose2d(\n",
        "        in_channels = 128,\n",
        "        out_channels = 64,\n",
        "        kernel_size = 5,\n",
        "        padding = 2\n",
        "    )\n",
        "    self.deconv2 = nn.ConvTranspose2d(\n",
        "        in_channels=64,\n",
        "        out_channels=1,\n",
        "        kernel_size=11,\n",
        "        padding=1,\n",
        "        stride=2\n",
        "    )\n",
        "    self.maxpool = nn.MaxPool2d(2, return_indices=True)\n",
        "    self.unpool = nn.MaxUnpool2d(2)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "    self.conv1(x)\n",
        "    self.maxpool(x)\n",
        "    self.relu(x)\n",
        "\n",
        "    self.conv2(x)\n",
        "    self.relu(x)\n",
        "\n",
        "    self.deconv1(x)\n",
        "    self.unpool(x)\n",
        "    self.relu(x)\n",
        "\n",
        "    self.deconv2(x)\n",
        "    self.relu(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtnGJaRkS1L5"
      },
      "outputs": [],
      "source": [
        "class DeepFont(nn.Module):\n",
        "  def __init__(self, num_channels, num_classes):\n",
        "    self.conv1 = nn.Conv2d(\n",
        "        in_channels=num_channels,\n",
        "        out_channels=64,\n",
        "        kernel_size=11,\n",
        "        padding=1,\n",
        "        stride=2\n",
        "    )\n",
        "    self.conv2 = nn.Conv2d(\n",
        "        in_channels=64,\n",
        "        out_channels=128,\n",
        "        kernel_size=5,\n",
        "        padding=2\n",
        "    )\n",
        "    self.conv3 = nn.Conv2d(\n",
        "        in_channels=128,\n",
        "        out_channels=256,\n",
        "        kernel_size=3,\n",
        "        padding=1\n",
        "    )\n",
        "    self.conv4 = nn.Conv2d(\n",
        "        in_channels=256,\n",
        "        out_channels=256,\n",
        "        kernel_size=3,\n",
        "        padding=1\n",
        "    )\n",
        "    self.conv5 = self.conv4\n",
        "    # need to apply dropout to fc6 and fc7\n",
        "    self.fc6 = nn.Linear(in_features=12*12*256, out_features=4096) # assuming input image size of 105. change in_feats for different sample size\n",
        "    self.fc7 = nn.Linear(in_features=4096, out_features=4096)\n",
        "    self.fc8 = nn.Linear(in_features=4096, out_features=num_classes)\n",
        "    self.norm1 = nn.BatchNorm2d(num_features=64)\n",
        "    self.norm2 = nn.BatchNorm2d(num_features=128)\n",
        "    self.dropout = nn.Dropout(0.5)\n",
        "    self.maxpool = nn.MaxPool2d(2)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.softmax = nn.CrossEntropyLoss()\n",
        "\n",
        "  def forward(self, x):\n",
        "    self.conv1(x)\n",
        "    self.norm1(x)\n",
        "    self.maxpool(x)\n",
        "    self.relu(x)\n",
        "\n",
        "    self.conv2(x)\n",
        "    self.norm2(x)\n",
        "    self.maxpool(x)\n",
        "    self.relu(x)\n",
        "\n",
        "    self.conv3(x)\n",
        "    self.relu(x)\n",
        "\n",
        "    self.conv4(x)\n",
        "    self.relu(x)\n",
        "\n",
        "    self.conv5(x)\n",
        "    self.relu(x)\n",
        "\n",
        "    self.flatten(x)\n",
        "\n",
        "    self.dropout(self.fc6(x))\n",
        "    self.relu(x)\n",
        "\n",
        "    self.dropout(self.fc7(x))\n",
        "    self.relu(x)\n",
        "\n",
        "    self.fc8(x)\n",
        "    self.relu(x)\n",
        "\n",
        "    self.softmax(x)\n",
        "\n",
        "    return x"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
