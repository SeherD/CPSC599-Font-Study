{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shutil\n",
        "import os\n",
        "from pathlib import Path\n",
        "import zipfile as zipfile\n",
        "\n",
        "\n",
        "# Destination directories for storing the downloaded data\n",
        "supervised_dest_dir = 'supervised'\n",
        "unsupervised_synthetic_dest_dir = 'unsupervised_synthetic'\n",
        "unsupervised_dest_dir = 'unsupervised'\n",
        "\n",
        "# Create destination directories if they don't exist\n",
        "os.makedirs(supervised_dest_dir, exist_ok=True)\n",
        "os.makedirs(unsupervised_synthetic_dest_dir, exist_ok=True)\n",
        "os.makedirs(unsupervised_dest_dir, exist_ok=True)\n",
        "\n",
        "# Function to download a folder from Google Drive\n",
        "def download_folder(zip_name,destination):\n",
        "\n",
        "    with zipfile.ZipFile(zip_name, 'r') as zip_ref:\n",
        "        zip_ref.extractall(destination)\n",
        "\n",
        "    # Move the contents of the extracted folder to the destination\n",
        "    extracted_folder = os.path.join(destination, Path(zip_name).stem)\n",
        "    for item in os.listdir(extracted_folder):\n",
        "        s = os.path.join(extracted_folder, item)\n",
        "        d = os.path.join(destination, item)\n",
        "        if os.path.isdir(s):\n",
        "            shutil.move(s, d)\n",
        "        else:\n",
        "            shutil.copy2(s, d)\n",
        "\n",
        "    # Clean up temporary files\n",
        "    #os.remove(zip_name)\n",
        "    shutil.rmtree(extracted_folder)\n",
        "\n",
        "# Download and organize the supervised dataset\n",
        "#download_folder('supervised_data.zip', supervised_dest_dir)\n",
        "\n",
        "# Download and organize the unsupervised dataset\n",
        "download_folder(\"unsupervised_data.zip\", unsupervised_dest_dir)\n",
        "\n",
        "#download_folder(\"unsupervised_synthetic_data.zip\", unsupervised_synthetic_dest_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Renamed: supervised/arial_images/arial-915(1).jpg to supervised/arial_images/arial-915.jpg\n",
            "Renamed: supervised/arial_images/arial-916(1).jpg to supervised/arial_images/arial-916.jpg\n",
            "Renamed: supervised/arial_images/arial-919(1).jpg to supervised/arial_images/arial-919.jpg\n",
            "Renamed: supervised/arial_images/arial-920(1).jpg to supervised/arial_images/arial-920.jpg\n",
            "Renamed: supervised/arial_images/arial-921(1).jpg to supervised/arial_images/arial-921.jpg\n",
            "Renamed: supervised/arial_images/arial-922(1).jpg to supervised/arial_images/arial-922.jpg\n",
            "Renamed: supervised/arial_images/arial-923(1).jpg to supervised/arial_images/arial-923.jpg\n",
            "Renamed: supervised/arial_images/arial-924(1).jpg to supervised/arial_images/arial-924.jpg\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import hashlib\n",
        "import shutil\n",
        "\n",
        "def get_file_checksum(file_path):\n",
        "    \"\"\"Calculate the checksum of a file.\"\"\"\n",
        "    sha256_hash = hashlib.sha256()\n",
        "    with open(file_path, \"rb\") as f:\n",
        "        # Read and update hash string value in blocks of 4K\n",
        "        for byte_block in iter(lambda: f.read(4096), b\"\"):\n",
        "            sha256_hash.update(byte_block)\n",
        "    return sha256_hash.hexdigest()\n",
        "\n",
        "def remove_duplicate_images(folder_path):\n",
        "    \"\"\"Remove duplicate images in a folder.\"\"\"\n",
        "    # Dictionary to store checksums and corresponding file paths\n",
        "    checksums = {}\n",
        "\n",
        "    # List all files in the folder\n",
        "    files = os.listdir(folder_path)\n",
        "\n",
        "    for file_name in files:\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "\n",
        "        # Check if the file is a regular file and not a directory\n",
        "        if os.path.isfile(file_path):\n",
        "            # Calculate the checksum of the file\n",
        "            checksum = get_file_checksum(file_path)\n",
        "\n",
        "            # Check if the checksum is already in the dictionary\n",
        "            if checksum in checksums:\n",
        "                # If a duplicate is found, remove the file\n",
        "                print(f\"Removing duplicate: {file_path}\")\n",
        "                os.remove(file_path)\n",
        "            else:\n",
        "                # Add the checksum to the dictionary\n",
        "                checksums[checksum] = file_path\n",
        "\n",
        "def rename_files(folder_path):\n",
        "    \"\"\"Rename files with '(1)' in their names.\"\"\"\n",
        "    # List all files in the folder\n",
        "    files = os.listdir(folder_path)\n",
        "\n",
        "    for file_name in files:\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "\n",
        "        # Check if the file is a regular file and not a directory\n",
        "        if os.path.isfile(file_path):\n",
        "            # Check if the file name contains '(1)'\n",
        "            if '(1)' in file_name:\n",
        "                # Rename the file by removing '(1)'\n",
        "                new_file_name = file_name.replace('(1)', '')\n",
        "                new_file_path = os.path.join(folder_path, new_file_name)\n",
        "                os.rename(file_path, new_file_path)\n",
        "                print(f\"Renamed: {file_path} to {new_file_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    folder_path = \"supervised/arial_images/\"\n",
        "    remove_duplicate_images(folder_path)\n",
        "    rename_files(folder_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "6zZVhUCht766"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dataset builder- Supervised"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1099\n",
            "1099\n",
            "1099\n",
            "1099\n",
            "1099\n",
            "1099\n",
            "1099\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "\n",
        "class SupervisedDataset(Dataset):\n",
        "    def __init__(self, root_dir):\n",
        "        self.root_dir = root_dir\n",
        "        self.image_paths = sorted(os.listdir(root_dir))  # Assuming images are directly in the root folder\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.root_dir, self.image_paths[idx])\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        # Assign labels based on your dataset structure\n",
        "        label = 0  # Replace with your actual labeling logic\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# Set your root directory\n",
        "root_dir = 'supervised'\n",
        "#subfolder = 'supervised_data'\n",
        "\n",
        "# Create datasets for each model\n",
        "models = ['arial', 'bradhitc', 'century_schoolbook', 'comic', 'cour', 'papyrus', 'times']\n",
        "train_datasets, val_datasets = [], []\n",
        "\n",
        "for model in models:\n",
        "    model_dir = os.path.join(root_dir, f\"{model}_images\")\n",
        "    all_data = SupervisedDataset(model_dir)\n",
        "    \n",
        "    # Split data into training and validation sets\n",
        "    train_size = 999\n",
        "    val_size = 100\n",
        "    train_data, val_data = torch.utils.data.random_split(all_data, [train_size, val_size])\n",
        "\n",
        "    train_datasets.append(train_data)\n",
        "    val_datasets.append(val_data)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loaders = [DataLoader(dataset, batch_size=32, shuffle=True) for dataset in train_datasets]\n",
        "val_loaders = [DataLoader(dataset, batch_size=32, shuffle=False) for dataset in val_datasets]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "32\n",
            "32\n",
            "32\n",
            "32\n",
            "32\n",
            "32\n",
            "32\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4lJFJUkovEba"
      },
      "outputs": [],
      "source": [
        "# Stacked Convolutional Auto-Encoder (the unsuperfvised sub-netwerork)\n",
        "class SCAE(nn.Module):\n",
        "  def __init__(self, num_channels, num_classes):\n",
        "    super.__init__()\n",
        "\n",
        "    self.conv1 = nn.Conv2d(\n",
        "        in_channels=num_channels,\n",
        "        out_channels=64,\n",
        "        kernel_size=11,\n",
        "        padding=1,\n",
        "        stride=2\n",
        "    )\n",
        "    self.conv2 = nn.Conv2d(\n",
        "        in_channels=64,\n",
        "        out_channels=128,\n",
        "        kernel_size=5,\n",
        "        padding=2\n",
        "    )\n",
        "    self.deconv1 = nn.ConvTranspose2d(\n",
        "        in_channels = 128,\n",
        "        out_channels = 64,\n",
        "        kernel_size = 5,\n",
        "        padding = 2\n",
        "    )\n",
        "    self.deconv2 = nn.ConvTranspose2d(\n",
        "        in_channels=64,\n",
        "        out_channels=1,\n",
        "        kernel_size=11,\n",
        "        padding=1,\n",
        "        stride=2\n",
        "    )\n",
        "    self.maxpool = nn.MaxPool2d(2, return_indices=True)\n",
        "    self.unpool = nn.MaxUnpool2d(2)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "    self.conv1(x)\n",
        "    self.maxpool(x)\n",
        "    self.relu(x)\n",
        "\n",
        "    self.conv2(x)\n",
        "    self.relu(x)\n",
        "\n",
        "    self.deconv1(x)\n",
        "    self.unpool(x)\n",
        "    self.relu(x)\n",
        "\n",
        "    self.deconv2(x)\n",
        "    self.relu(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtnGJaRkS1L5"
      },
      "outputs": [],
      "source": [
        "class DeepFont(nn.Module):\n",
        "  def __init__(self, num_channels, num_classes):\n",
        "    self.conv1 = nn.Conv2d(\n",
        "        in_channels=num_channels,\n",
        "        out_channels=64,\n",
        "        kernel_size=11,\n",
        "        padding=1,\n",
        "        stride=2\n",
        "    )\n",
        "    self.conv2 = nn.Conv2d(\n",
        "        in_channels=64,\n",
        "        out_channels=128,\n",
        "        kernel_size=5,\n",
        "        padding=2\n",
        "    )\n",
        "    self.conv3 = nn.Conv2d(\n",
        "        in_channels=128,\n",
        "        out_channels=256,\n",
        "        kernel_size=3,\n",
        "        padding=1\n",
        "    )\n",
        "    self.conv4 = nn.Conv2d(\n",
        "        in_channels=256,\n",
        "        out_channels=256,\n",
        "        kernel_size=3,\n",
        "        padding=1\n",
        "    )\n",
        "    self.conv5 = self.conv4\n",
        "    # need to apply dropout to fc6 and fc7\n",
        "    self.fc6 = nn.Linear(in_features=12*12*256, out_features=4096) # assuming input image size of 105. change in_feats for different sample size\n",
        "    self.fc7 = nn.Linear(in_features=4096, out_features=4096)\n",
        "    self.fc8 = nn.Linear(in_features=4096, out_features=num_classes)\n",
        "    self.norm1 = nn.BatchNorm2d(num_features=64)\n",
        "    self.norm2 = nn.BatchNorm2d(num_features=128)\n",
        "    self.dropout = nn.Dropout(0.5)\n",
        "    self.maxpool = nn.MaxPool2d(2)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.softmax = nn.CrossEntropyLoss()\n",
        "\n",
        "  def forward(self, x):\n",
        "    self.conv1(x)\n",
        "    self.norm1(x)\n",
        "    self.maxpool(x)\n",
        "    self.relu(x)\n",
        "\n",
        "    self.conv2(x)\n",
        "    self.norm2(x)\n",
        "    self.maxpool(x)\n",
        "    self.relu(x)\n",
        "\n",
        "    self.conv3(x)\n",
        "    self.relu(x)\n",
        "\n",
        "    self.conv4(x)\n",
        "    self.relu(x)\n",
        "\n",
        "    self.conv5(x)\n",
        "    self.relu(x)\n",
        "\n",
        "    self.flatten(x)\n",
        "\n",
        "    self.dropout(self.fc6(x))\n",
        "    self.relu(x)\n",
        "\n",
        "    self.dropout(self.fc7(x))\n",
        "    self.relu(x)\n",
        "\n",
        "    self.fc8(x)\n",
        "    self.relu(x)\n",
        "\n",
        "    self.softmax(x)\n",
        "\n",
        "    return x"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
