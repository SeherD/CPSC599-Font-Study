{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'unsupervised_data.zip'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\P2L Inc\\Documents\\CPSC599A3\\CPSC599-Font-Study\\computer_vision_model.ipynb Cell 2\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/P2L%20Inc/Documents/CPSC599A3/CPSC599-Font-Study/computer_vision_model.ipynb#W1sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     shutil\u001b[39m.\u001b[39mrmtree(extracted_folder)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/P2L%20Inc/Documents/CPSC599A3/CPSC599-Font-Study/computer_vision_model.ipynb#W1sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39m# Download and organize the supervised dataset\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/P2L%20Inc/Documents/CPSC599A3/CPSC599-Font-Study/computer_vision_model.ipynb#W1sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39m#download_folder('supervised_data.zip', supervised_dest_dir)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/P2L%20Inc/Documents/CPSC599A3/CPSC599-Font-Study/computer_vision_model.ipynb#W1sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/P2L%20Inc/Documents/CPSC599A3/CPSC599-Font-Study/computer_vision_model.ipynb#W1sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39m# Download and organize the unsupervised dataset\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/P2L%20Inc/Documents/CPSC599A3/CPSC599-Font-Study/computer_vision_model.ipynb#W1sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m download_folder(\u001b[39m\"\u001b[39;49m\u001b[39munsupervised_data.zip\u001b[39;49m\u001b[39m\"\u001b[39;49m, unsupervised_dest_dir)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/P2L%20Inc/Documents/CPSC599A3/CPSC599-Font-Study/computer_vision_model.ipynb#W1sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39m#download_folder(\"unsupervised_synthetic_data.zip\", unsupervised_synthetic_dest_dir)\u001b[39;00m\n",
            "\u001b[1;32mc:\\Users\\P2L Inc\\Documents\\CPSC599A3\\CPSC599-Font-Study\\computer_vision_model.ipynb Cell 2\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/P2L%20Inc/Documents/CPSC599A3/CPSC599-Font-Study/computer_vision_model.ipynb#W1sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdownload_folder\u001b[39m(zip_name,destination):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/P2L%20Inc/Documents/CPSC599A3/CPSC599-Font-Study/computer_vision_model.ipynb#W1sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39mwith\u001b[39;00m zipfile\u001b[39m.\u001b[39;49mZipFile(zip_name, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m zip_ref:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/P2L%20Inc/Documents/CPSC599A3/CPSC599-Font-Study/computer_vision_model.ipynb#W1sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m         zip_ref\u001b[39m.\u001b[39mextractall(destination)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/P2L%20Inc/Documents/CPSC599A3/CPSC599-Font-Study/computer_vision_model.ipynb#W1sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39m# Move the contents of the extracted folder to the destination\u001b[39;00m\n",
            "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.1776.0_x64__qbz5n2kfra8p0\\Lib\\zipfile.py:1284\u001b[0m, in \u001b[0;36mZipFile.__init__\u001b[1;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps, metadata_encoding)\u001b[0m\n\u001b[0;32m   1282\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m   1283\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1284\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39;49mopen(file, filemode)\n\u001b[0;32m   1285\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[0;32m   1286\u001b[0m         \u001b[39mif\u001b[39;00m filemode \u001b[39min\u001b[39;00m modeDict:\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'unsupervised_data.zip'"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "import os\n",
        "from pathlib import Path\n",
        "import zipfile as zipfile\n",
        "\n",
        "\n",
        "# Destination directories for storing the downloaded data\n",
        "supervised_dest_dir = 'supervised'\n",
        "unsupervised_synthetic_dest_dir = 'unsupervised_synthetic'\n",
        "unsupervised_dest_dir = 'unsupervised'\n",
        "\n",
        "# Create destination directories if they don't exist\n",
        "os.makedirs(supervised_dest_dir, exist_ok=True)\n",
        "os.makedirs(unsupervised_synthetic_dest_dir, exist_ok=True)\n",
        "os.makedirs(unsupervised_dest_dir, exist_ok=True)\n",
        "\n",
        "# Function to download a folder from Google Drive\n",
        "def download_folder(zip_name,destination):\n",
        "\n",
        "    with zipfile.ZipFile(zip_name, 'r') as zip_ref:\n",
        "        zip_ref.extractall(destination)\n",
        "\n",
        "    # Move the contents of the extracted folder to the destination\n",
        "    extracted_folder = os.path.join(destination, Path(zip_name).stem)\n",
        "    for item in os.listdir(extracted_folder):\n",
        "        s = os.path.join(extracted_folder, item)\n",
        "        d = os.path.join(destination, item)\n",
        "        if os.path.isdir(s):\n",
        "            shutil.move(s, d)\n",
        "        else:\n",
        "            shutil.copy2(s, d)\n",
        "\n",
        "    # Clean up temporary files\n",
        "    #os.remove(zip_name)\n",
        "    shutil.rmtree(extracted_folder)\n",
        "\n",
        "# Download and organize the supervised dataset\n",
        "#download_folder('supervised_data.zip', supervised_dest_dir)\n",
        "\n",
        "# Download and organize the unsupervised dataset\n",
        "download_folder(\"unsupervised_data.zip\", unsupervised_dest_dir)\n",
        "\n",
        "#download_folder(\"unsupervised_synthetic_data.zip\", unsupervised_synthetic_dest_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import hashlib\n",
        "import shutil\n",
        "\n",
        "def get_file_checksum(file_path):\n",
        "    \"\"\"Calculate the checksum of a file.\"\"\"\n",
        "    sha256_hash = hashlib.sha256()\n",
        "    with open(file_path, \"rb\") as f:\n",
        "        # Read and update hash string value in blocks of 4K\n",
        "        for byte_block in iter(lambda: f.read(4096), b\"\"):\n",
        "            sha256_hash.update(byte_block)\n",
        "    return sha256_hash.hexdigest()\n",
        "\n",
        "def remove_duplicate_images(folder_path):\n",
        "    \"\"\"Remove duplicate images in a folder.\"\"\"\n",
        "    # Dictionary to store checksums and corresponding file paths\n",
        "    checksums = {}\n",
        "\n",
        "    # List all files in the folder\n",
        "    files = os.listdir(folder_path)\n",
        "\n",
        "    for file_name in files:\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "\n",
        "        # Check if the file is a regular file and not a directory\n",
        "        if os.path.isfile(file_path):\n",
        "            # Calculate the checksum of the file\n",
        "            checksum = get_file_checksum(file_path)\n",
        "\n",
        "            # Check if the checksum is already in the dictionary\n",
        "            if checksum in checksums:\n",
        "                # If a duplicate is found, remove the file\n",
        "                print(f\"Removing duplicate: {file_path}\")\n",
        "                os.remove(file_path)\n",
        "            else:\n",
        "                # Add the checksum to the dictionary\n",
        "                checksums[checksum] = file_path\n",
        "\n",
        "def rename_files(folder_path):\n",
        "    \"\"\"Rename files with '(1)' in their names.\"\"\"\n",
        "    # List all files in the folder\n",
        "    files = os.listdir(folder_path)\n",
        "\n",
        "    for file_name in files:\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "\n",
        "        # Check if the file is a regular file and not a directory\n",
        "        if os.path.isfile(file_path):\n",
        "            # Check if the file name contains '(1)'\n",
        "            if '(1)' in file_name:\n",
        "                # Rename the file by removing '(1)'\n",
        "                new_file_name = file_name.replace('(1)', '')\n",
        "                new_file_path = os.path.join(folder_path, new_file_name)\n",
        "                os.rename(file_path, new_file_path)\n",
        "                print(f\"Renamed: {file_path} to {new_file_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    folder_path = r\"unsupervised_synthetic\\times_alphabet_images_rotated\"\n",
        "    remove_duplicate_images(folder_path)\n",
        "    #rename_files(folder_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6zZVhUCht766"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dataset builder- Supervised"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def word_to_tensor(word):\n",
        "    letter_count = [0] * 52\n",
        "    for char in word:\n",
        "        if 'a' <= char <= 'z':\n",
        "            letter_count[ord(char) - ord('a')] += 1\n",
        "        elif 'A' <= char <= 'Z':\n",
        "            letter_count[ord(char) - ord('A') + 26] += 1\n",
        "    return torch.tensor(letter_count, dtype=torch.float32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "class SupervisedDataset(Dataset):\n",
        "    def __init__(self, root_dir, labels_path,transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.labels_path = labels_path\n",
        "        self.data = []\n",
        "        self.transform=transform\n",
        "        with open(labels_path, newline=\"\") as labels_file:\n",
        "            labels_reader = csv.reader(labels_file)\n",
        "            for row in labels_reader:\n",
        "                self.data.append(row)  # a list of [filename, [chars in image]]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.root_dir, self.data[idx][0])\n",
        "\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image= self.transform(image)\n",
        "        label_text = self.data[idx][1]\n",
        "\n",
        "        # Convert label text to array of letter counts\n",
        "        label_tensor = word_to_tensor(label_text)\n",
        "\n",
        "        return image, label_tensor\n",
        "\n",
        "# Define the transform\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "# Set your root directory\n",
        "root_dir = \"supervised\"\n",
        "# subfolder = 'supervised_data'\n",
        "\n",
        "# Create datasets for each model\n",
        "models = [\n",
        "    \"arial\",\n",
        "    \"bradhitc\",\n",
        "    \"century_schoolbook\",\n",
        "    \"comic\",\n",
        "    \"cour\",\n",
        "    \"papyrus\",\n",
        "    \"times\",\n",
        "]\n",
        "train_datasets, val_datasets = [], []\n",
        "transform = transforms.Compose([\n",
        "            \n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "for model in models:\n",
        "    model_dir = os.path.join(root_dir, f\"{model}_images\")\n",
        "    labels_path = os.path.join(root_dir, f\"{model}.csv\")\n",
        "    all_data = SupervisedDataset(model_dir, labels_path,transform)\n",
        "\n",
        "    # Split data into training and validation sets\n",
        "    train_size = 1000\n",
        "    val_size = 100\n",
        "    train_data, val_data = torch.utils.data.random_split(\n",
        "        all_data, [train_size, val_size]\n",
        "    )\n",
        "\n",
        "    train_datasets.append(train_data)\n",
        "    val_datasets.append(val_data)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loaders = [\n",
        "    DataLoader(dataset, batch_size=16, shuffle=True) for dataset in train_datasets\n",
        "]\n",
        "val_loaders = [\n",
        "    DataLoader(dataset, batch_size=16, shuffle=False) for dataset in val_datasets\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dataset builder - Unsupervised"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of samples in the unsupervised dataloader: 7956\n",
            "Number of samples in the validation dataloader: 884\n"
          ]
        }
      ],
      "source": [
        "from torchvision import transforms\n",
        "from torch.utils.data import ConcatDataset, Dataset, DataLoader, Subset\n",
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class CustomFontDataset(Dataset):\n",
        "    def __init__(self, root, transform=None):\n",
        "        self.root = root\n",
        "\n",
        "        self.rwlabel = 0\n",
        "        self.synlabel = 1\n",
        "        \n",
        "        self.transform = transform\n",
        "        self.unsupervised_data = self.load_unsupervised_data()\n",
        "        self.synthetic_data = self.load_synthetic_data()\n",
        "\n",
        "        # use this for stratifying the data\n",
        "        self.img_labels = [self.rwlabel] * len(self.unsupervised_data) + [self.synlabel] * len(self.synthetic_data)\n",
        "\n",
        "    def load_unsupervised_data(self):\n",
        "        unsupervised_path = os.path.join(self.root, 'unsupervised')\n",
        "        unsupervised_images = [os.path.join(unsupervised_path, img) for img in os.listdir(unsupervised_path)]\n",
        "        return unsupervised_images\n",
        "\n",
        "    def load_synthetic_data(self):\n",
        "        synthetic_path = os.path.join(self.root, f'unsupervised_synthetic')\n",
        "        synthetic_images = [os.path.join(synthetic_path, img) for img in os.listdir(synthetic_path)]\n",
        "        # actually only take every third one\n",
        "        return synthetic_images[0::3]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.unsupervised_data) + len(self.synthetic_data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if index < len(self.unsupervised_data):\n",
        "            img_path = self.unsupervised_data[index]\n",
        "            label = self.rwlabel  # set the label for real world unsupervised data\n",
        "        else:\n",
        "            adjusted_index = index - len(self.unsupervised_data)\n",
        "            img_path = self.synthetic_data[adjusted_index]\n",
        "            label = self.synlabel  # set the label for synthetic data\n",
        "\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        \n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# Define the transform \n",
        "transform = transforms.Compose([\n",
        "            transforms.Resize((64, 64)),\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "\n",
        "# Set root to the current working directory\n",
        "current_working_directory = os.getcwd()\n",
        "\n",
        "# Create datasets for each font\n",
        "dataset = CustomFontDataset(root=current_working_directory, transform=transform)\n",
        "\n",
        "# Split unsupervised data into train and val\n",
        "train_idx, validation_idx = train_test_split(np.arange(len(dataset)),\n",
        "                                             test_size=0.1,\n",
        "                                             random_state=999,\n",
        "                                             shuffle=True,\n",
        "                                             stratify=dataset.img_labels)\n",
        "\n",
        "# Subset dataset for train and val\n",
        "train_dataset = Subset(dataset, train_idx)\n",
        "validation_dataset = Subset(dataset, validation_idx)\n",
        "\n",
        "# Create a dataloader for the dataset\n",
        "batch_size = 32\n",
        "\n",
        "# Dataloader for train and val\n",
        "unsupervised_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "unsupervised_val_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Access the combined dataloader\n",
        "print(f\"Number of samples in the unsupervised dataloader: {len(unsupervised_loader.dataset)}\")\n",
        "print(f\"Number of samples in the validation dataloader: {len(unsupervised_val_loader.dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4lJFJUkovEba"
      },
      "outputs": [],
      "source": [
        "# Stacked Convolutional Auto-Encoder (the unsupervised sub-network)\n",
        "class SCAE(nn.Module):\n",
        "  def __init__(self, num_channels):\n",
        "    super().__init__()\n",
        "\n",
        "    self.conv1 = nn.Conv2d(\n",
        "        in_channels=num_channels,\n",
        "        out_channels=64,\n",
        "        kernel_size=11,\n",
        "        padding=1,\n",
        "        stride=2\n",
        "    )\n",
        "    self.conv2 = nn.Conv2d(\n",
        "        in_channels=64,\n",
        "        out_channels=128,\n",
        "        kernel_size=5,\n",
        "        padding=2\n",
        "    )\n",
        "    self.deconv1 = nn.ConvTranspose2d(\n",
        "        in_channels = 128,\n",
        "        out_channels = 64,\n",
        "        kernel_size = 5,\n",
        "        padding = 2\n",
        "    )\n",
        "    self.deconv2 = nn.ConvTranspose2d(\n",
        "        in_channels=64,\n",
        "        # out channels should be the same as in_channels\n",
        "        out_channels=num_channels,\n",
        "        kernel_size=11,\n",
        "        padding=1,\n",
        "        # using stride in the conv1 layer means that multiple input sizes are mapped to the same size\n",
        "        # output_padding of 1 ensures that the output is the same size as the input\n",
        "        # in the specific case that the model is producing an output 1 smaller than the input in both dimensions\n",
        "        # change the output padding value if you change the input image size\n",
        "        output_padding=1,\n",
        "        stride=2\n",
        "    )\n",
        "    self.maxpool = nn.MaxPool2d(2, return_indices=True)\n",
        "    self.unpool = nn.MaxUnpool2d(2)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    x1 = self.conv1(x)\n",
        "    x2 = self.relu(x1)\n",
        "    x3, indices = self.maxpool(x2)\n",
        "\n",
        "    x4 = self.conv2(x3)\n",
        "    x5 = self.relu(x4)\n",
        "\n",
        "    x6 = self.deconv1(x5)\n",
        "    x7 = self.unpool(x6, indices, output_size=x2.size())\n",
        "    x8 = self.relu(x7)\n",
        "\n",
        "    x9 = self.deconv2(x8)\n",
        "    x10 = self.relu(x9)\n",
        "\n",
        "    return x10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "PtnGJaRkS1L5"
      },
      "outputs": [],
      "source": [
        "class DeepFont(nn.Module):\n",
        "  def __init__(self, num_channels, num_classes):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.conv1 = nn.Conv2d(\n",
        "        in_channels=num_channels,\n",
        "        out_channels=64,\n",
        "        kernel_size=11,\n",
        "        padding=1,\n",
        "        stride=2\n",
        "    )\n",
        "    self.conv2 = nn.Conv2d(\n",
        "        in_channels=64,\n",
        "        out_channels=128,\n",
        "        kernel_size=5,\n",
        "        padding=2\n",
        "    )\n",
        "    self.conv3 = nn.Conv2d(\n",
        "        in_channels=128,\n",
        "        out_channels=256,\n",
        "        kernel_size=3,\n",
        "        padding=1\n",
        "    )\n",
        "    self.conv4 = nn.Conv2d(\n",
        "        in_channels=256,\n",
        "        out_channels=256,\n",
        "        kernel_size=3,\n",
        "        padding=1\n",
        "    )\n",
        "    self.conv5 = nn.Conv2d(\n",
        "        in_channels=256,\n",
        "        out_channels=256,\n",
        "        kernel_size=3,\n",
        "        padding=1\n",
        "    )\n",
        "    self.fc6 = nn.Linear(in_features=31*31*256, out_features=4096) # assuming input image size of 256x256. change in_feats for different sample size\n",
        "    self.fc7 = nn.Linear(in_features=4096, out_features=4096)\n",
        "    self.fc8 = nn.Linear(in_features=4096, out_features=num_classes)\n",
        "    self.norm1 = nn.BatchNorm2d(num_features=64)\n",
        "    self.norm2 = nn.BatchNorm2d(num_features=128)\n",
        "    self.dropout = nn.Dropout(0.5)\n",
        "    self.maxpool = nn.MaxPool2d(2)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.flatten = nn.Flatten()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x =self.conv1(x)\n",
        "    x = self.norm1(x)\n",
        "    x = self.maxpool(x)\n",
        "    x = self.relu(x)\n",
        "\n",
        "    x = self.conv2(x)\n",
        "    x = self.norm2(x)\n",
        "    x = self.maxpool(x)\n",
        "    x = self.relu(x)\n",
        "\n",
        "    x = self.conv3(x)\n",
        "    x = self.relu(x)\n",
        "\n",
        "    x = self.conv4(x)\n",
        "    x = self.relu(x)\n",
        "\n",
        "    x = self.conv5(x)\n",
        "    x = self.relu(x)\n",
        "\n",
        "    x = self.flatten(x)\n",
        "\n",
        "    x = self.dropout(self.fc6(x))\n",
        "    x = self.relu(x)\n",
        "\n",
        "    x = self.dropout(self.fc7(x))\n",
        "    x = self.relu(x)\n",
        "\n",
        "    x = self.fc8(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def training_unsupervised(model, dataloader, criterion, optimizer, device, epochs, model_path):\n",
        "    model = model.to(device)\n",
        "    model.train()\n",
        "    best_loss = torch.inf\n",
        "    for i in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch_index, (images, _) in enumerate(dataloader):\n",
        "            optimizer.zero_grad()\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, images)\n",
        "            total_loss += loss.item() * images.size(0)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        avg_loss = total_loss / (batch_index+1)\n",
        "        print(f'Epoch {i} | Avg Loss: {avg_loss:.4f} | Total Loss: {total_loss:.4f}')\n",
        "        if avg_loss < best_loss:\n",
        "            best_loss = avg_loss\n",
        "            torch.save(model.state_dict(), model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "def evaluate_unsupervised(model, dataloader, device):\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_index, (images, _) in enumerate(dataloader):\n",
        "            images = images.to(device)\n",
        "            \n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            \n",
        "            # Calculate reconstruction loss (MSE)\n",
        "            loss = F.mse_loss(outputs, images)\n",
        "            \n",
        "            total_loss += loss.item() * images.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader.dataset)\n",
        "    print(f'Validation Loss: {avg_loss:.4f}')\n",
        "    return avg_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluation(model, dataloader, criterion, device, phase='Validation'):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    ground_truth = []\n",
        "\n",
        "    true_positives = 0\n",
        "    true_negatives = 0\n",
        "    false_positives = 0\n",
        "    false_negatives = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        total_loss = 0\n",
        "        total_samples = 0\n",
        "\n",
        "        for _, (images, labels) in enumerate(dataloader):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            total_loss += loss.item() * images.size(0)\n",
        "            total_samples += images.size(0)\n",
        "\n",
        "            # Convert output probabilities to binary predictions\n",
        "            preds = (torch.sigmoid(outputs) > 0.5).float()\n",
        "\n",
        "            # Update multi-label metrics\n",
        "            true_positives += (preds * labels).sum().item()\n",
        "            true_negatives += ((1 - labels) * (1 - preds)).sum().item()\n",
        "            false_positives += ((1 - labels) * preds).sum().item()\n",
        "            false_negatives += (labels * (1 - preds)).sum().item()\n",
        "\n",
        "            predictions.extend(preds.cpu().numpy())\n",
        "            ground_truth.extend(labels.cpu().numpy())\n",
        "\n",
        "        # Calculate multi-label metrics\n",
        "        precision = true_positives / (true_positives + false_positives + 1e-10)\n",
        "        recall = true_positives / (true_positives + false_negatives + 1e-10)\n",
        "        f1_score = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
        "\n",
        "        accuracy = (true_positives + true_negatives) / (total_samples + 1e-10)\n",
        "        loss = total_loss / total_samples\n",
        "\n",
        "        print(f'{phase}\\tF1-Score={f1_score:<10.4f}' +\n",
        "              f'\\t\\tLoss= {loss:<10.4f}' +\n",
        "              f'\\t\\tPrecision: {precision:<10.4f}' +\n",
        "              f'\\t\\tRecall: {recall:<10.4f}' +\n",
        "              f'\\t\\tAccuracy: {accuracy:<10.4f}')\n",
        "\n",
        "        return {'loss': loss,\n",
        "                'f1_score': f1_score,\n",
        "                'precision': precision,\n",
        "                'recall': recall,\n",
        "                'accuracy': accuracy,\n",
        "                'ground_truth': ground_truth,\n",
        "                'predictions': predictions}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def training_supervised(model, train_loader, val_loader, criterion, optimizer, device, epochs, best_model_path):\n",
        "    model = model.to(device)\n",
        "    model.train()\n",
        "    best_loss = torch.inf\n",
        "    best_results = None\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        total_samples = 0\n",
        "\n",
        "        # New variables for multi-label metrics\n",
        "        true_positives = 0\n",
        "        true_negatives = 0\n",
        "        false_positives = 0\n",
        "        false_negatives = 0\n",
        "\n",
        "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item() * images.size(0)\n",
        "            total_samples += images.size(0)\n",
        "\n",
        "            # Convert output probabilities to binary predictions\n",
        "            preds = (torch.sigmoid(outputs) > 0.5).float()\n",
        "\n",
        "            # Update multi-label metrics\n",
        "            true_positives += (preds * labels).sum().item()\n",
        "            true_negatives += ((1 - labels) * (1 - preds)).sum().item()\n",
        "            false_positives += ((1 - labels) * preds).sum().item()\n",
        "            false_negatives += (labels * (1 - preds)).sum().item()\n",
        "\n",
        "        # Calculate multi-label metrics\n",
        "        precision = true_positives / (true_positives + false_positives + 1e-10)\n",
        "        recall = true_positives / (true_positives + false_negatives + 1e-10)\n",
        "        f1_score = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
        "\n",
        "        accuracy = (true_positives + true_negatives) / (total_samples + 1e-10)\n",
        "        loss = total_loss / total_samples\n",
        "\n",
        "        print(f'{epoch:<4}\\tTrain\\tF1-Score={f1_score:<10.4f}' +\n",
        "              f'\\t\\tLoss= {loss:<10.4f}' +\n",
        "              f'\\t\\tPrecision: {precision:<10.4f}' +\n",
        "              f'\\t\\tRecall: {recall:<10.4f}' +\n",
        "              f'\\t\\tAccuracy: {accuracy:<10.4f}')\n",
        "\n",
        "        results = evaluation(model, val_loader, criterion, device)\n",
        "\n",
        "        if results['loss'] < best_loss:\n",
        "            torch.save(model.state_dict(), best_model_path)\n",
        "            best_loss = results['loss']\n",
        "            best_results = results\n",
        "\n",
        "        print()\n",
        "\n",
        "    return best_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import os\n",
        "#os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"512\"\n",
        "#os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
        "#torch.cuda.empty_cache()\n",
        "models_dir = 'models'\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "# Train the unsupervised sub-network\n",
        "# Hyperparameters\n",
        "learning_rate = 0.01\n",
        "momentum = 0.9\n",
        "weight_decay = 5e-4\n",
        "epochs = 20\n",
        "\n",
        "scae_model = SCAE(num_channels=3)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.SGD(scae_model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
        "\n",
        "model_path = os.path.join(models_dir, f\"SCAE.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 | Avg Loss: 2.9112 | Total Loss: 724.8937\n",
            "Epoch 1 | Avg Loss: 1.4500 | Total Loss: 361.0467\n",
            "Epoch 2 | Avg Loss: 1.1622 | Total Loss: 289.3810\n",
            "Epoch 3 | Avg Loss: 0.9416 | Total Loss: 234.4570\n",
            "Epoch 4 | Avg Loss: 0.8296 | Total Loss: 206.5784\n",
            "Epoch 5 | Avg Loss: 0.7505 | Total Loss: 186.8727\n",
            "Epoch 6 | Avg Loss: 0.7054 | Total Loss: 175.6546\n",
            "Epoch 7 | Avg Loss: 0.6603 | Total Loss: 164.4263\n",
            "Epoch 8 | Avg Loss: 0.6307 | Total Loss: 157.0462\n",
            "Epoch 9 | Avg Loss: 0.6080 | Total Loss: 151.3987\n"
          ]
        }
      ],
      "source": [
        "# train the SCAE\n",
        "training_unsupervised(scae_model, unsupervised_loader, criterion, optimizer, device, epochs, model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Loss: 0.0183\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.018342625231764435"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# evaluate the SCAE\n",
        "evaluate_unsupervised(scae_model, unsupervised_val_loader, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "papyrus\n",
            "0   \tTrain\tF1-Score=0.1823    \t\tLoss= 0.6933    \t\tPrecision: 0.1214    \t\tRecall: 0.3658    \t\tAccuracy: 26.2230   \n",
            "Validation\tF1-Score=0.1865    \t\tLoss= 0.6933    \t\tPrecision: 0.1272    \t\tRecall: 0.3494    \t\tAccuracy: 27.9300   \n",
            "\n",
            "1   \tTrain\tF1-Score=0.1728    \t\tLoss= 0.6934    \t\tPrecision: 0.1182    \t\tRecall: 0.3211    \t\tAccuracy: 27.8450   \n",
            "Validation\tF1-Score=0.1865    \t\tLoss= 0.6933    \t\tPrecision: 0.1272    \t\tRecall: 0.3494    \t\tAccuracy: 27.9300   \n",
            "\n",
            "2   \tTrain\tF1-Score=0.1728    \t\tLoss= 0.6934    \t\tPrecision: 0.1182    \t\tRecall: 0.3211    \t\tAccuracy: 27.8450   \n",
            "Validation\tF1-Score=0.1865    \t\tLoss= 0.6933    \t\tPrecision: 0.1272    \t\tRecall: 0.3494    \t\tAccuracy: 27.9300   \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Train the supervised sub-network\n",
        "models_dir = 'models'\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "# Train the unsupervised sub-network\n",
        "# Hyperparameters\n",
        "learning_rate = 0.01\n",
        "momentum = 0.9\n",
        "weight_decay = 5e-4\n",
        "epochs = 10\n",
        "\n",
        "supervised_model = DeepFont(num_channels=3, num_classes=52) # one class per letter (case-sensitive)\n",
        "\n",
        "# Import the convolutional layers of the SCAE as conv1 and conv2\n",
        "scae_path = os.path.join(models_dir, f\"SCAE.pt\")\n",
        "supervised_model.load_state_dict(torch.load(scae_path), strict=False)\n",
        "\n",
        "# Freeze the convolutional layers from SCAE\n",
        "for param in supervised_model.conv1.parameters():\n",
        "    param.requires_grad = False\n",
        "for param in supervised_model.conv2.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "i=5\n",
        "font_name = models[i]\n",
        "print(font_name)\n",
        "    \n",
        "train_loader = train_loaders[i]\n",
        "val_loader = val_loaders[i]\n",
        "best_model_path = os.path.join(models_dir, f\"{font_name}_model.pt\")\n",
        "best_results = training_supervised(supervised_model, train_loader, val_loader, criterion, optimizer, device, epochs, best_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Testing our models\n",
        "\n",
        "models_dir = 'models'\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "for i in range(len(models)):\n",
        "    font_name = models[i]\n",
        "    print(font_name)\n",
        "    model_path = os.path.join(models_dir, f\"{font_name}_model.pt\")\n",
        "    model = torch.load(model_path, map_location=device)\n",
        "    results = evaluation(model, test_loader, criterion, device, 'Test')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
