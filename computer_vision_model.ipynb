{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shutil\n",
        "import os\n",
        "from pathlib import Path\n",
        "import zipfile as zipfile\n",
        "\n",
        "\n",
        "# Destination directories for storing the downloaded data\n",
        "supervised_dest_dir = 'supervised'\n",
        "unsupervised_synthetic_dest_dir = 'unsupervised_synthetic'\n",
        "unsupervised_dest_dir = 'unsupervised'\n",
        "\n",
        "# Create destination directories if they don't exist\n",
        "os.makedirs(supervised_dest_dir, exist_ok=True)\n",
        "os.makedirs(unsupervised_synthetic_dest_dir, exist_ok=True)\n",
        "os.makedirs(unsupervised_dest_dir, exist_ok=True)\n",
        "\n",
        "# Function to download a folder from Google Drive\n",
        "def download_folder(zip_name,destination):\n",
        "\n",
        "    with zipfile.ZipFile(zip_name, 'r') as zip_ref:\n",
        "        zip_ref.extractall(destination)\n",
        "\n",
        "    # Move the contents of the extracted folder to the destination\n",
        "    extracted_folder = os.path.join(destination, Path(zip_name).stem)\n",
        "    for item in os.listdir(extracted_folder):\n",
        "        s = os.path.join(extracted_folder, item)\n",
        "        d = os.path.join(destination, item)\n",
        "        if os.path.isdir(s):\n",
        "            shutil.move(s, d)\n",
        "        else:\n",
        "            shutil.copy2(s, d)\n",
        "\n",
        "    # Clean up temporary files\n",
        "    #os.remove(zip_name)\n",
        "    shutil.rmtree(extracted_folder)\n",
        "\n",
        "# Download and organize the supervised dataset\n",
        "#download_folder('supervised_data.zip', supervised_dest_dir)\n",
        "\n",
        "# Download and organize the unsupervised dataset\n",
        "download_folder(\"unsupervised_data.zip\", unsupervised_dest_dir)\n",
        "\n",
        "#download_folder(\"unsupervised_synthetic_data.zip\", unsupervised_synthetic_dest_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import hashlib\n",
        "import shutil\n",
        "\n",
        "def get_file_checksum(file_path):\n",
        "    \"\"\"Calculate the checksum of a file.\"\"\"\n",
        "    sha256_hash = hashlib.sha256()\n",
        "    with open(file_path, \"rb\") as f:\n",
        "        # Read and update hash string value in blocks of 4K\n",
        "        for byte_block in iter(lambda: f.read(4096), b\"\"):\n",
        "            sha256_hash.update(byte_block)\n",
        "    return sha256_hash.hexdigest()\n",
        "\n",
        "def remove_duplicate_images(folder_path):\n",
        "    \"\"\"Remove duplicate images in a folder.\"\"\"\n",
        "    # Dictionary to store checksums and corresponding file paths\n",
        "    checksums = {}\n",
        "\n",
        "    # List all files in the folder\n",
        "    files = os.listdir(folder_path)\n",
        "\n",
        "    for file_name in files:\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "\n",
        "        # Check if the file is a regular file and not a directory\n",
        "        if os.path.isfile(file_path):\n",
        "            # Calculate the checksum of the file\n",
        "            checksum = get_file_checksum(file_path)\n",
        "\n",
        "            # Check if the checksum is already in the dictionary\n",
        "            if checksum in checksums:\n",
        "                # If a duplicate is found, remove the file\n",
        "                print(f\"Removing duplicate: {file_path}\")\n",
        "                os.remove(file_path)\n",
        "            else:\n",
        "                # Add the checksum to the dictionary\n",
        "                checksums[checksum] = file_path\n",
        "\n",
        "def rename_files(folder_path):\n",
        "    \"\"\"Rename files with '(1)' in their names.\"\"\"\n",
        "    # List all files in the folder\n",
        "    files = os.listdir(folder_path)\n",
        "\n",
        "    for file_name in files:\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "\n",
        "        # Check if the file is a regular file and not a directory\n",
        "        if os.path.isfile(file_path):\n",
        "            # Check if the file name contains '(1)'\n",
        "            if '(1)' in file_name:\n",
        "                # Rename the file by removing '(1)'\n",
        "                new_file_name = file_name.replace('(1)', '')\n",
        "                new_file_path = os.path.join(folder_path, new_file_name)\n",
        "                os.rename(file_path, new_file_path)\n",
        "                print(f\"Renamed: {file_path} to {new_file_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    folder_path = r\"unsupervised_synthetic\\times_alphabet_images_rotated\"\n",
        "    remove_duplicate_images(folder_path)\n",
        "    #rename_files(folder_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6zZVhUCht766"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dataset builder- Supervised"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def word_to_tensor(word):\n",
        "    letter_count = [0] * 52\n",
        "    for char in word:\n",
        "        if 'a' <= char <= 'z':\n",
        "            letter_count[ord(char) - ord('a')] += 1\n",
        "        elif 'A' <= char <= 'Z':\n",
        "            letter_count[ord(char) - ord('A') + 26] += 1\n",
        "    return torch.tensor(letter_count, dtype=torch.float32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "class SupervisedDataset(Dataset):\n",
        "    def __init__(self, root_dir, labels_path):\n",
        "        self.root_dir = root_dir\n",
        "        self.labels_path = labels_path\n",
        "        self.data = []\n",
        "        with open(labels_path, newline=\"\") as labels_file:\n",
        "            labels_reader = csv.reader(labels_file)\n",
        "            for row in labels_reader:\n",
        "                self.data.append(row)  # a list of [filename, [chars in image]]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.root_dir, self.data[idx][0])\n",
        "\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        label_text = self.data[idx][1]\n",
        "\n",
        "        # Convert label text to array of letter counts\n",
        "        label_tensor = word_to_tensor(label_text)\n",
        "\n",
        "        return image, label_tensor\n",
        "\n",
        "\n",
        "# Set your root directory\n",
        "root_dir = \"supervised\"\n",
        "# subfolder = 'supervised_data'\n",
        "\n",
        "# Create datasets for each model\n",
        "models = [\n",
        "    \"arial\",\n",
        "    \"bradhitc\",\n",
        "    \"century_schoolbook\",\n",
        "    \"comic\",\n",
        "    \"cour\",\n",
        "    \"papyrus\",\n",
        "    \"times\",\n",
        "]\n",
        "train_datasets, val_datasets = [], []\n",
        "\n",
        "for model in models:\n",
        "    model_dir = os.path.join(root_dir, f\"{model}_images\")\n",
        "    labels_path = os.path.join(root_dir, f\"{model}.csv\")\n",
        "    all_data = SupervisedDataset(model_dir, labels_path)\n",
        "\n",
        "    # Split data into training and validation sets\n",
        "    train_size = 1000\n",
        "    val_size = 100\n",
        "    train_data, val_data = torch.utils.data.random_split(\n",
        "        all_data, [train_size, val_size]\n",
        "    )\n",
        "\n",
        "    train_datasets.append(train_data)\n",
        "    val_datasets.append(val_data)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loaders = [\n",
        "    DataLoader(dataset, batch_size=32, shuffle=True) for dataset in train_datasets\n",
        "]\n",
        "val_loaders = [\n",
        "    DataLoader(dataset, batch_size=32, shuffle=False) for dataset in val_datasets\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dataset builder - Unsupervised"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of samples in the combined dataloader: 16120\n"
          ]
        }
      ],
      "source": [
        "from torchvision import transforms\n",
        "from torch.utils.data import ConcatDataset, Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "class CustomFontDataset(Dataset):\n",
        "    def __init__(self, root, transform=None):\n",
        "        self.root = root\n",
        "        \n",
        "        self.transform = transform\n",
        "        self.unsupervised_data = self.load_unsupervised_data()\n",
        "        self.synthetic_data = self.load_synthetic_data()\n",
        "\n",
        "    def load_unsupervised_data(self):\n",
        "        unsupervised_path = os.path.join(self.root, 'unsupervised')\n",
        "        unsupervised_images = [os.path.join(unsupervised_path, img) for img in os.listdir(unsupervised_path)]\n",
        "        return unsupervised_images\n",
        "\n",
        "    def load_synthetic_data(self):\n",
        "        synthetic_path = os.path.join(self.root, f'unsupervised_synthetic')\n",
        "        synthetic_images = [os.path.join(synthetic_path, img) for img in os.listdir(synthetic_path)]\n",
        "        return synthetic_images\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.unsupervised_data) + len(self.synthetic_data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if index < len(self.unsupervised_data):\n",
        "            img_path = self.unsupervised_data[index]\n",
        "            label = 0  # You can set the label for unsupervised data to 0 or any other value\n",
        "        else:\n",
        "            adjusted_index = index - len(self.unsupervised_data)\n",
        "            img_path = self.synthetic_data[adjusted_index]\n",
        "            label = 1  # You can set the label for synthetic data to 1 or any other value\n",
        "\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        \n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# Define the transform (you can customize this based on your needs)\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "transform = transforms.Compose([\n",
        "            transforms.Resize((50, 50)),\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "# Set root to the current working directory\n",
        "current_working_directory = os.getcwd()\n",
        "\n",
        "# Create datasets for each font\n",
        "dataset = CustomFontDataset(root=current_working_directory, transform=transform)\n",
        "\n",
        "# Concatenate all datasets into one\n",
        "\n",
        "\n",
        "# Create a dataloader for the combined dataset\n",
        "batch_size = 32\n",
        "unsupervised_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Access the combined dataloader\n",
        "print(f\"Number of samples in the combined dataloader: {len(unsupervised_loader.dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4lJFJUkovEba"
      },
      "outputs": [],
      "source": [
        "# Stacked Convolutional Auto-Encoder (the unsupervised sub-network)\n",
        "class SCAE(nn.Module):\n",
        "  def __init__(self, num_channels):\n",
        "    super().__init__()\n",
        "\n",
        "    self.conv1 = nn.Conv2d(\n",
        "        in_channels=num_channels,\n",
        "        out_channels=64,\n",
        "        kernel_size=11,\n",
        "        padding=1,\n",
        "        stride=2\n",
        "    )\n",
        "    self.conv2 = nn.Conv2d(\n",
        "        in_channels=64,\n",
        "        out_channels=128,\n",
        "        kernel_size=5,\n",
        "        padding=2\n",
        "    )\n",
        "    self.deconv1 = nn.ConvTranspose2d(\n",
        "        in_channels = 128,\n",
        "        out_channels = 64,\n",
        "        kernel_size = 5,\n",
        "        padding = 2\n",
        "    )\n",
        "    self.deconv2 = nn.ConvTranspose2d(\n",
        "        in_channels=64,\n",
        "        out_channels=1,\n",
        "        kernel_size=11,\n",
        "        padding=1,\n",
        "        # using stride in the conv1 layer means that multiple input sizes are mapped to the same size\n",
        "        # output_padding of 1 ensures that the output is the same size as the input\n",
        "        # in the specific case that the model is producing an output 1 smaller than the input in both dimensions\n",
        "        # change the output padding value if you change the input image size\n",
        "        output_padding=1,\n",
        "        stride=2\n",
        "    )\n",
        "    self.maxpool = nn.MaxPool2d(2, return_indices=True)\n",
        "    self.unpool = nn.MaxUnpool2d(2)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    x1 = self.conv1(x)\n",
        "    x2 = self.relu(x1)\n",
        "    x3, indices = self.maxpool(x2)\n",
        "\n",
        "    x4 = self.conv2(x3)\n",
        "    x5 = self.relu(x4)\n",
        "\n",
        "    x6 = self.deconv1(x5)\n",
        "    x7 = self.unpool(x6, indices, output_size=x2.size())\n",
        "    x8 = self.relu(x7)\n",
        "\n",
        "    x9 = self.deconv2(x8)\n",
        "    x10 = self.relu(x9)\n",
        "\n",
        "    return x10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "PtnGJaRkS1L5"
      },
      "outputs": [],
      "source": [
        "class DeepFont(nn.Module):\n",
        "  def __init__(self, num_channels, num_classes):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.conv1 = nn.Conv2d(\n",
        "        in_channels=num_channels,\n",
        "        out_channels=64,\n",
        "        kernel_size=11,\n",
        "        padding=1,\n",
        "        stride=2\n",
        "    )\n",
        "    self.conv2 = nn.Conv2d(\n",
        "        in_channels=64,\n",
        "        out_channels=128,\n",
        "        kernel_size=5,\n",
        "        padding=2\n",
        "    )\n",
        "    self.conv3 = nn.Conv2d(\n",
        "        in_channels=128,\n",
        "        out_channels=256,\n",
        "        kernel_size=3,\n",
        "        padding=1\n",
        "    )\n",
        "    self.conv4 = nn.Conv2d(\n",
        "        in_channels=256,\n",
        "        out_channels=256,\n",
        "        kernel_size=3,\n",
        "        padding=1\n",
        "    )\n",
        "    self.conv5 = nn.Conv2d(\n",
        "        in_channels=256,\n",
        "        out_channels=256,\n",
        "        kernel_size=3,\n",
        "        padding=1\n",
        "    )\n",
        "    self.fc6 = nn.Linear(in_features=12*12*256, out_features=4096) # assuming input image size of 28x28. change in_feats for different sample size\n",
        "    self.fc7 = nn.Linear(in_features=4096, out_features=4096)\n",
        "    self.fc8 = nn.Linear(in_features=4096, out_features=num_classes)\n",
        "    self.norm1 = nn.BatchNorm2d(num_features=64)\n",
        "    self.norm2 = nn.BatchNorm2d(num_features=128)\n",
        "    self.dropout = nn.Dropout(0.5)\n",
        "    self.maxpool = nn.MaxPool2d(2)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.flatten = nn.Flatten()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x =self.conv1(x)\n",
        "    x = self.norm1(x)\n",
        "    x = self.maxpool(x)\n",
        "    x = self.relu(x)\n",
        "\n",
        "    x = self.conv2(x)\n",
        "    x = self.norm2(x)\n",
        "    x = self.maxpool(x)\n",
        "    x = self.relu(x)\n",
        "\n",
        "    x = self.conv3(x)\n",
        "    x = self.relu(x)\n",
        "\n",
        "    x = self.conv4(x)\n",
        "    x = self.relu(x)\n",
        "\n",
        "    x = self.conv5(x)\n",
        "    x = self.relu(x)\n",
        "\n",
        "    x = self.flatten(x)\n",
        "\n",
        "    x = self.dropout(self.fc6(x))\n",
        "    x = self.relu(x)\n",
        "\n",
        "    x = self.dropout(self.fc7(x))\n",
        "    x = self.relu(x)\n",
        "\n",
        "    x = self.fc8(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def sensitivity(preds, labels, num_classes=52):\n",
        "    scores = []\n",
        "    \n",
        "    for value in range(num_classes):\n",
        "        mask = labels == value\n",
        "        tp = (preds[mask] == value).sum().item()\n",
        "        fn = (preds[mask] != value).sum().item()\n",
        "        \n",
        "        if tp + fn > 0:\n",
        "            scores.append(tp / (tp + fn))\n",
        "    \n",
        "    return sum(scores) / len(scores) if len(scores) > 0 else 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def specificity(preds, labels, num_classes=52):\n",
        "    scores = []\n",
        "    \n",
        "    for value in range(num_classes):\n",
        "        mask = labels != value\n",
        "        tn = (preds[mask] != value).sum().item()\n",
        "        fp = (preds[mask] == value).sum().item()\n",
        "        \n",
        "        if tn + fp > 0:\n",
        "            scores.append(tn / (tn + fp))\n",
        "    \n",
        "    return sum(scores) / len(scores) if len(scores) > 0 else 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def precision(preds, labels, num_classes=52):\n",
        "    scores = []\n",
        "    \n",
        "    for value in range(num_classes):\n",
        "        mask = preds == value\n",
        "        tp = (labels[mask] == value).sum().item()\n",
        "        fp = (labels[mask] != value).sum().item()\n",
        "        \n",
        "        if tp + fp > 0:\n",
        "            scores.append(tp / (tp + fp))\n",
        "    \n",
        "    return sum(scores) / len(scores) if len(scores) > 0 else 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def training_unsupervised(model, dataloader, criterion, optimizer, device, epochs, model_path):\n",
        "    model = model.to(device)\n",
        "    model.train()\n",
        "    best_loss = torch.inf\n",
        "    for _ in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch_index, (images, _) in enumerate(dataloader):\n",
        "            optimizer.zero_grad()\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, images)\n",
        "            total_loss += loss.item() * images.size(0)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        avg_loss = total_loss / (batch_index+1)\n",
        "        if avg_loss < best_loss:\n",
        "            best_loss = avg_loss\n",
        "            torch.save(model.state_dict(), model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluation(model, dataloader, criterion, device, phase='Validation'):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    ground_truth = []\n",
        "    with torch.no_grad():\n",
        "        total_loss = 0\n",
        "        total = 0\n",
        "        correct = 0\n",
        "        for _, (images, labels) in enumerate(dataloader):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item() * images.size(0)\n",
        "            total += images.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            predictions.extend(preds.cpu().numpy())\n",
        "            ground_truth.extend(labels.cpu().numpy())\n",
        "            correct += (preds == labels).sum().item()\n",
        "\n",
        "        accuracy = correct / total\n",
        "        loss = total_loss / total\n",
        "        sensitivity_score = sensitivity(torch.tensor(predictions), torch.tensor(ground_truth))\n",
        "        specificity_score = specificity(torch.tensor(predictions), torch.tensor(ground_truth))\n",
        "        precision_score = precision(torch.tensor(predictions), torch.tensor(ground_truth))\n",
        "\n",
        "        print(f'\\t{phase}\\tAccuracy={accuracy:<10.4f}' +\n",
        "              f'\\t\\tLoss= {loss:<10.4f}' +\n",
        "              f'\\t\\tSensitivity: {sensitivity_score:<10.4f}' +\n",
        "              f'\\t\\tSpecificity: {specificity_score:<10.4f}' +\n",
        "              f'\\t\\tPrecision: {precision_score:<10.4f}')\n",
        "\n",
        "        return {'loss': loss,\n",
        "                'accuracy': accuracy,\n",
        "                'sensitivity': sensitivity_score,\n",
        "                'specificity': specificity_score,\n",
        "                'precision': precision_score,\n",
        "                'ground_truth': ground_truth,\n",
        "                'predictions': predictions}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Supervised model training function adapted from code provided by Dr. Farhad Maleki in MNISTFasion_CNN.ipynb\n",
        "def training_supervised(model, train_loader, val_loader, criterion, optimizer, device, epochs, best_model_path):\n",
        "    model = model.to(device)\n",
        "    model.train()\n",
        "    best_loss = torch.inf\n",
        "    best_results = None\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        total = 0\n",
        "        correct = 0\n",
        "\n",
        "        for _, (images, labels) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item() * images.size(0)\n",
        "            total += images.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "\n",
        "        accuracy = correct / total\n",
        "        loss = total_loss / total\n",
        "\n",
        "        # Adapt the metrics functions to handle tensors\n",
        "        sensitivity_score = sensitivity(preds, labels)\n",
        "        specificity_score = specificity(preds, labels)\n",
        "        precision_score = precision(preds, labels)\n",
        "\n",
        "        print(f'{epoch:<4}\\tTrain\\tAccuracy={accuracy:<10.4f}' +\n",
        "              f'\\t\\tLoss= {loss:<10.4f}' +\n",
        "              f'\\t\\tSensitivity: {sensitivity_score:<10.4f}' +\n",
        "              f'\\t\\tSpecificity: {specificity_score:<10.4f}' +\n",
        "              f'\\t\\tPrecision: {precision_score:<10.4f}')\n",
        "\n",
        "        results = evaluation(model, val_loader, criterion, device)\n",
        "        \n",
        "        if results['loss'] < best_loss:\n",
        "            torch.save(model, best_model_path)\n",
        "            best_loss = results['loss']\n",
        "            best_results = results\n",
        "\n",
        "        print()\n",
        "\n",
        "    return best_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Found an invalid max index: 418 (output volumes are of size 20x20",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\seher\\Documents\\GitHub\\CPSC599-Font-Study\\computer_vision_model.ipynb Cell 18\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/seher/Documents/GitHub/CPSC599-Font-Study/computer_vision_model.ipynb#Y125sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mSGD(scae_model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mlearning_rate, momentum\u001b[39m=\u001b[39mmomentum, weight_decay\u001b[39m=\u001b[39mweight_decay)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/seher/Documents/GitHub/CPSC599-Font-Study/computer_vision_model.ipynb#Y125sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m model_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(models_dir, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSCAE.pt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/seher/Documents/GitHub/CPSC599-Font-Study/computer_vision_model.ipynb#Y125sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m training_unsupervised(scae_model, unsupervised_loader, criterion, optimizer, device, epochs, model_path)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/seher/Documents/GitHub/CPSC599-Font-Study/computer_vision_model.ipynb#Y125sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# Train the supervised sub-network\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/seher/Documents/GitHub/CPSC599-Font-Study/computer_vision_model.ipynb#Y125sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m supervised_model \u001b[39m=\u001b[39m DeepFont(num_channels\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, num_classes\u001b[39m=\u001b[39m\u001b[39m52\u001b[39m) \u001b[39m# one class per letter (case-sensitive)\u001b[39;00m\n",
            "\u001b[1;32mc:\\Users\\seher\\Documents\\GitHub\\CPSC599-Font-Study\\computer_vision_model.ipynb Cell 18\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/seher/Documents/GitHub/CPSC599-Font-Study/computer_vision_model.ipynb#Y125sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/seher/Documents/GitHub/CPSC599-Font-Study/computer_vision_model.ipynb#Y125sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m images \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/seher/Documents/GitHub/CPSC599-Font-Study/computer_vision_model.ipynb#Y125sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(images)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/seher/Documents/GitHub/CPSC599-Font-Study/computer_vision_model.ipynb#Y125sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, images)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/seher/Documents/GitHub/CPSC599-Font-Study/computer_vision_model.ipynb#Y125sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem() \u001b[39m*\u001b[39m images\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "\u001b[1;32mc:\\Users\\seher\\Documents\\GitHub\\CPSC599-Font-Study\\computer_vision_model.ipynb Cell 18\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/seher/Documents/GitHub/CPSC599-Font-Study/computer_vision_model.ipynb#Y125sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/seher/Documents/GitHub/CPSC599-Font-Study/computer_vision_model.ipynb#Y125sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeconv1(x)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/seher/Documents/GitHub/CPSC599-Font-Study/computer_vision_model.ipynb#Y125sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49munpool(x, indices)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/seher/Documents/GitHub/CPSC599-Font-Study/computer_vision_model.ipynb#Y125sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/seher/Documents/GitHub/CPSC599-Font-Study/computer_vision_model.ipynb#Y125sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeconv2(x)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\pooling.py:412\u001b[0m, in \u001b[0;36mMaxUnpool2d.forward\u001b[1;34m(self, input, indices, output_size)\u001b[0m\n\u001b[0;32m    411\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, indices: Tensor, output_size: Optional[List[\u001b[39mint\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 412\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mmax_unpool2d(\u001b[39minput\u001b[39;49m, indices, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkernel_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    413\u001b[0m                           \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, output_size)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\functional.py:985\u001b[0m, in \u001b[0;36mmax_unpool2d\u001b[1;34m(input, indices, kernel_size, stride, padding, output_size)\u001b[0m\n\u001b[0;32m    983\u001b[0m padding \u001b[39m=\u001b[39m _pair(padding)\n\u001b[0;32m    984\u001b[0m output_size \u001b[39m=\u001b[39m _unpool_output_size(\u001b[39minput\u001b[39m, kernel_size, _stride, padding, output_size)\n\u001b[1;32m--> 985\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mmax_unpool2d(\u001b[39minput\u001b[39;49m, indices, output_size)\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Found an invalid max index: 418 (output volumes are of size 20x20"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "models_dir = 'models'\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "# Train the unsupervised sub-network\n",
        "# Hyperparameters\n",
        "learning_rate = 0.01\n",
        "momentum = 0.9\n",
        "weight_decay = 5e-4\n",
        "epochs = 10\n",
        "\n",
        "scae_model = SCAE(num_channels=3)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.SGD(scae_model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
        "\n",
        "model_path = os.path.join(models_dir, f\"SCAE.pt\")\n",
        "training_unsupervised(scae_model, unsupervised_loader, criterion, optimizer, device, epochs, model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the supervised sub-network\n",
        "supervised_model = DeepFont(num_channels=3, num_classes=52) # one class per letter (case-sensitive)\n",
        "\n",
        "# Import the convolutional layers of the SCAE as conv1 and conv2\n",
        "scae_path = os.path.join(models_dir, f\"SCAE.pt\")\n",
        "supervised_model.load_state_dict(torch.load(scae_path), strict=False)\n",
        "\n",
        "# Freeze the convolutional layers from SCAE\n",
        "for param in supervised_model.conv1.parameters():\n",
        "    param.requires_grad = False\n",
        "for param in supervised_model.conv2.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "for i in range(len(models)):\n",
        "    font_name = models[i]\n",
        "    print(font_name)\n",
        "    \n",
        "    train_loader = train_loaders[i]\n",
        "    val_loader = val_loaders[i]\n",
        "    best_model_path = os.path.join(models_dir, f\"{font_name}_model.pt\")\n",
        "    best_results = training_supervised(supervised_model, train_loader, val_loader, criterion, optimizer, device, epochs, best_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Testing our models\n",
        "\n",
        "models_dir = 'models'\n",
        "criterion = nn.MSELoss()\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "for i in range(len(models)):\n",
        "    font_name = models[i]\n",
        "    print(font_name)\n",
        "    model_path = os.path.join(models_dir, f\"{font_name}_model.pt\")\n",
        "    model = torch.load(model_path)\n",
        "    results = evaluation(model, test_loader, criterion, device, 'Test')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
