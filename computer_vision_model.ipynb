{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'unsupervised_data.zip'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\P2L Inc\\Documents\\CPSC599A3\\CPSC599-Font-Study\\computer_vision_model.ipynb Cell 2\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/P2L%20Inc/Documents/CPSC599A3/CPSC599-Font-Study/computer_vision_model.ipynb#W1sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     shutil\u001b[39m.\u001b[39mrmtree(extracted_folder)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/P2L%20Inc/Documents/CPSC599A3/CPSC599-Font-Study/computer_vision_model.ipynb#W1sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39m# Download and organize the supervised dataset\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/P2L%20Inc/Documents/CPSC599A3/CPSC599-Font-Study/computer_vision_model.ipynb#W1sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39m#download_folder('supervised_data.zip', supervised_dest_dir)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/P2L%20Inc/Documents/CPSC599A3/CPSC599-Font-Study/computer_vision_model.ipynb#W1sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/P2L%20Inc/Documents/CPSC599A3/CPSC599-Font-Study/computer_vision_model.ipynb#W1sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39m# Download and organize the unsupervised dataset\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/P2L%20Inc/Documents/CPSC599A3/CPSC599-Font-Study/computer_vision_model.ipynb#W1sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m download_folder(\u001b[39m\"\u001b[39;49m\u001b[39munsupervised_data.zip\u001b[39;49m\u001b[39m\"\u001b[39;49m, unsupervised_dest_dir)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/P2L%20Inc/Documents/CPSC599A3/CPSC599-Font-Study/computer_vision_model.ipynb#W1sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39m#download_folder(\"unsupervised_synthetic_data.zip\", unsupervised_synthetic_dest_dir)\u001b[39;00m\n",
            "\u001b[1;32mc:\\Users\\P2L Inc\\Documents\\CPSC599A3\\CPSC599-Font-Study\\computer_vision_model.ipynb Cell 2\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/P2L%20Inc/Documents/CPSC599A3/CPSC599-Font-Study/computer_vision_model.ipynb#W1sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdownload_folder\u001b[39m(zip_name,destination):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/P2L%20Inc/Documents/CPSC599A3/CPSC599-Font-Study/computer_vision_model.ipynb#W1sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39mwith\u001b[39;00m zipfile\u001b[39m.\u001b[39;49mZipFile(zip_name, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m zip_ref:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/P2L%20Inc/Documents/CPSC599A3/CPSC599-Font-Study/computer_vision_model.ipynb#W1sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m         zip_ref\u001b[39m.\u001b[39mextractall(destination)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/P2L%20Inc/Documents/CPSC599A3/CPSC599-Font-Study/computer_vision_model.ipynb#W1sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39m# Move the contents of the extracted folder to the destination\u001b[39;00m\n",
            "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.1776.0_x64__qbz5n2kfra8p0\\Lib\\zipfile.py:1284\u001b[0m, in \u001b[0;36mZipFile.__init__\u001b[1;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps, metadata_encoding)\u001b[0m\n\u001b[0;32m   1282\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m   1283\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1284\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39;49mopen(file, filemode)\n\u001b[0;32m   1285\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[0;32m   1286\u001b[0m         \u001b[39mif\u001b[39;00m filemode \u001b[39min\u001b[39;00m modeDict:\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'unsupervised_data.zip'"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "import os\n",
        "from pathlib import Path\n",
        "import zipfile as zipfile\n",
        "\n",
        "\n",
        "# Destination directories for storing the downloaded data\n",
        "supervised_dest_dir = 'supervised'\n",
        "unsupervised_synthetic_dest_dir = 'unsupervised_synthetic'\n",
        "unsupervised_dest_dir = 'unsupervised'\n",
        "\n",
        "# Create destination directories if they don't exist\n",
        "os.makedirs(supervised_dest_dir, exist_ok=True)\n",
        "os.makedirs(unsupervised_synthetic_dest_dir, exist_ok=True)\n",
        "os.makedirs(unsupervised_dest_dir, exist_ok=True)\n",
        "\n",
        "# Function to download a folder from Google Drive\n",
        "def download_folder(zip_name,destination):\n",
        "\n",
        "    with zipfile.ZipFile(zip_name, 'r') as zip_ref:\n",
        "        zip_ref.extractall(destination)\n",
        "\n",
        "    # Move the contents of the extracted folder to the destination\n",
        "    extracted_folder = os.path.join(destination, Path(zip_name).stem)\n",
        "    for item in os.listdir(extracted_folder):\n",
        "        s = os.path.join(extracted_folder, item)\n",
        "        d = os.path.join(destination, item)\n",
        "        if os.path.isdir(s):\n",
        "            shutil.move(s, d)\n",
        "        else:\n",
        "            shutil.copy2(s, d)\n",
        "\n",
        "    # Clean up temporary files\n",
        "    #os.remove(zip_name)\n",
        "    shutil.rmtree(extracted_folder)\n",
        "\n",
        "# Download and organize the supervised dataset\n",
        "#download_folder('supervised_data.zip', supervised_dest_dir)\n",
        "\n",
        "# Download and organize the unsupervised dataset\n",
        "download_folder(\"unsupervised_data.zip\", unsupervised_dest_dir)\n",
        "\n",
        "#download_folder(\"unsupervised_synthetic_data.zip\", unsupervised_synthetic_dest_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import hashlib\n",
        "import shutil\n",
        "\n",
        "def get_file_checksum(file_path):\n",
        "    \"\"\"Calculate the checksum of a file.\"\"\"\n",
        "    sha256_hash = hashlib.sha256()\n",
        "    with open(file_path, \"rb\") as f:\n",
        "        # Read and update hash string value in blocks of 4K\n",
        "        for byte_block in iter(lambda: f.read(4096), b\"\"):\n",
        "            sha256_hash.update(byte_block)\n",
        "    return sha256_hash.hexdigest()\n",
        "\n",
        "def remove_duplicate_images(folder_path):\n",
        "    \"\"\"Remove duplicate images in a folder.\"\"\"\n",
        "    # Dictionary to store checksums and corresponding file paths\n",
        "    checksums = {}\n",
        "\n",
        "    # List all files in the folder\n",
        "    files = os.listdir(folder_path)\n",
        "\n",
        "    for file_name in files:\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "\n",
        "        # Check if the file is a regular file and not a directory\n",
        "        if os.path.isfile(file_path):\n",
        "            # Calculate the checksum of the file\n",
        "            checksum = get_file_checksum(file_path)\n",
        "\n",
        "            # Check if the checksum is already in the dictionary\n",
        "            if checksum in checksums:\n",
        "                # If a duplicate is found, remove the file\n",
        "                print(f\"Removing duplicate: {file_path}\")\n",
        "                os.remove(file_path)\n",
        "            else:\n",
        "                # Add the checksum to the dictionary\n",
        "                checksums[checksum] = file_path\n",
        "\n",
        "def rename_files(folder_path):\n",
        "    \"\"\"Rename files with '(1)' in their names.\"\"\"\n",
        "    # List all files in the folder\n",
        "    files = os.listdir(folder_path)\n",
        "\n",
        "    for file_name in files:\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "\n",
        "        # Check if the file is a regular file and not a directory\n",
        "        if os.path.isfile(file_path):\n",
        "            # Check if the file name contains '(1)'\n",
        "            if '(1)' in file_name:\n",
        "                # Rename the file by removing '(1)'\n",
        "                new_file_name = file_name.replace('(1)', '')\n",
        "                new_file_path = os.path.join(folder_path, new_file_name)\n",
        "                os.rename(file_path, new_file_path)\n",
        "                print(f\"Renamed: {file_path} to {new_file_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    folder_path = r\"unsupervised_synthetic\\times_alphabet_images_rotated\"\n",
        "    remove_duplicate_images(folder_path)\n",
        "    #rename_files(folder_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6zZVhUCht766"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dataset builder- Supervised"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def word_to_tensor(word):\n",
        "    letter_count = [0] * 52\n",
        "    for char in word:\n",
        "        if 'a' <= char <= 'z':\n",
        "            letter_count[ord(char) - ord('a')] += 1\n",
        "        elif 'A' <= char <= 'Z':\n",
        "            letter_count[ord(char) - ord('A') + 26] += 1\n",
        "    return torch.tensor(letter_count, dtype=torch.float32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\P2L Inc\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] The specified procedure could not be found'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
            "  warn(\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import csv\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "class SupervisedDataset(Dataset):\n",
        "    def __init__(self, root_dir, labels_path,transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.labels_path = labels_path\n",
        "        self.data = []\n",
        "        self.transform=transform\n",
        "        with open(labels_path, newline=\"\") as labels_file:\n",
        "            labels_reader = csv.reader(labels_file)\n",
        "            for row in labels_reader:\n",
        "                self.data.append(row)  # a list of [filename, [chars in image]]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.root_dir, self.data[idx][0])\n",
        "\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image= self.transform(image)\n",
        "        label_text = self.data[idx][1]\n",
        "\n",
        "        # Convert label text to array of letter counts\n",
        "        label_tensor = word_to_tensor(label_text)\n",
        "\n",
        "        return image, label_tensor\n",
        "\n",
        "# Define the transform\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "# Set your root directory\n",
        "root_dir = \"supervised\"\n",
        "# subfolder = 'supervised_data'\n",
        "\n",
        "# Create datasets for each model\n",
        "models = [\n",
        "    \"arial\",\n",
        "    \"bradhitc\",\n",
        "    \"century_schoolbook\",\n",
        "    \"comic\",\n",
        "    \"cour\",\n",
        "    \"papyrus\",\n",
        "    \"times\",\n",
        "]\n",
        "train_datasets, val_datasets = [], []\n",
        "transform = transforms.Compose([\n",
        "            \n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "for model in models:\n",
        "    model_dir = os.path.join(root_dir, f\"{model}_images\")\n",
        "    labels_path = os.path.join(root_dir, f\"{model}.csv\")\n",
        "    all_data = SupervisedDataset(model_dir, labels_path,transform)\n",
        "\n",
        "    # Split data into training and validation sets\n",
        "    train_size = 1000\n",
        "    val_size = 100\n",
        "    train_data, val_data = torch.utils.data.random_split(\n",
        "        all_data, [train_size, val_size]\n",
        "    )\n",
        "\n",
        "    train_datasets.append(train_data)\n",
        "    val_datasets.append(val_data)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loaders = [\n",
        "    DataLoader(dataset, batch_size=32, shuffle=True) for dataset in train_datasets\n",
        "]\n",
        "val_loaders = [\n",
        "    DataLoader(dataset, batch_size=32, shuffle=False) for dataset in val_datasets\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dataset builder - Unsupervised"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of samples in the combined dataloader: 16120\n"
          ]
        }
      ],
      "source": [
        "from torchvision import transforms\n",
        "from torch.utils.data import ConcatDataset, Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "class CustomFontDataset(Dataset):\n",
        "    def __init__(self, root, transform=None):\n",
        "        self.root = root\n",
        "        \n",
        "        self.transform = transform\n",
        "        self.unsupervised_data = self.load_unsupervised_data()\n",
        "        self.synthetic_data = self.load_synthetic_data()\n",
        "\n",
        "    def load_unsupervised_data(self):\n",
        "        unsupervised_path = os.path.join(self.root, 'unsupervised')\n",
        "        unsupervised_images = [os.path.join(unsupervised_path, img) for img in os.listdir(unsupervised_path)]\n",
        "        return unsupervised_images\n",
        "\n",
        "    def load_synthetic_data(self):\n",
        "        synthetic_path = os.path.join(self.root, f'unsupervised_synthetic')\n",
        "        synthetic_images = [os.path.join(synthetic_path, img) for img in os.listdir(synthetic_path)]\n",
        "        return synthetic_images\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.unsupervised_data) + len(self.synthetic_data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if index < len(self.unsupervised_data):\n",
        "            img_path = self.unsupervised_data[index]\n",
        "            label = 0  # You can set the label for unsupervised data to 0 or any other value\n",
        "        else:\n",
        "            adjusted_index = index - len(self.unsupervised_data)\n",
        "            img_path = self.synthetic_data[adjusted_index]\n",
        "            label = 1  # You can set the label for synthetic data to 1 or any other value\n",
        "\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        \n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# Define the transform (you can customize this based on your needs)\n",
        "transform = transforms.Compose([\n",
        "            transforms.Resize((64, 64)),\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "# Set root to the current working directory\n",
        "current_working_directory = os.getcwd()\n",
        "\n",
        "# Create datasets for each font\n",
        "dataset = CustomFontDataset(root=current_working_directory, transform=transform)\n",
        "\n",
        "# Concatenate all datasets into one\n",
        "\n",
        "\n",
        "# Create a dataloader for the combined dataset\n",
        "batch_size = 32\n",
        "unsupervised_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Access the combined dataloader\n",
        "print(f\"Number of samples in the combined dataloader: {len(unsupervised_loader.dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4lJFJUkovEba"
      },
      "outputs": [],
      "source": [
        "# Stacked Convolutional Auto-Encoder (the unsupervised sub-network)\n",
        "class SCAE(nn.Module):\n",
        "  def __init__(self, num_channels):\n",
        "    super().__init__()\n",
        "\n",
        "    self.conv1 = nn.Conv2d(\n",
        "        in_channels=num_channels,\n",
        "        out_channels=64,\n",
        "        kernel_size=11,\n",
        "        padding=1,\n",
        "        stride=2\n",
        "    )\n",
        "    self.conv2 = nn.Conv2d(\n",
        "        in_channels=64,\n",
        "        out_channels=128,\n",
        "        kernel_size=5,\n",
        "        padding=2\n",
        "    )\n",
        "    self.deconv1 = nn.ConvTranspose2d(\n",
        "        in_channels = 128,\n",
        "        out_channels = 64,\n",
        "        kernel_size = 5,\n",
        "        padding = 2\n",
        "    )\n",
        "    self.deconv2 = nn.ConvTranspose2d(\n",
        "        in_channels=64,\n",
        "        out_channels=1,\n",
        "        kernel_size=11,\n",
        "        padding=1,\n",
        "        # using stride in the conv1 layer means that multiple input sizes are mapped to the same size\n",
        "        # output_padding of 1 ensures that the output is the same size as the input\n",
        "        # in the specific case that the model is producing an output 1 smaller than the input in both dimensions\n",
        "        # change the output padding value if you change the input image size\n",
        "        output_padding=1,\n",
        "        stride=2\n",
        "    )\n",
        "    self.maxpool = nn.MaxPool2d(2, return_indices=True)\n",
        "    self.unpool = nn.MaxUnpool2d(2)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    x1 = self.conv1(x)\n",
        "    x2 = self.relu(x1)\n",
        "    x3, indices = self.maxpool(x2)\n",
        "\n",
        "    x4 = self.conv2(x3)\n",
        "    x5 = self.relu(x4)\n",
        "\n",
        "    x6 = self.deconv1(x5)\n",
        "    x7 = self.unpool(x6, indices, output_size=x2.size())\n",
        "    x8 = self.relu(x7)\n",
        "\n",
        "    x9 = self.deconv2(x8)\n",
        "    x10 = self.relu(x9)\n",
        "\n",
        "    return x10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "PtnGJaRkS1L5"
      },
      "outputs": [],
      "source": [
        "class DeepFont(nn.Module):\n",
        "  def __init__(self, num_channels, num_classes):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.conv1 = nn.Conv2d(\n",
        "        in_channels=num_channels,\n",
        "        out_channels=64,\n",
        "        kernel_size=11,\n",
        "        padding=1,\n",
        "        stride=2\n",
        "    )\n",
        "    self.conv2 = nn.Conv2d(\n",
        "        in_channels=64,\n",
        "        out_channels=128,\n",
        "        kernel_size=5,\n",
        "        padding=2\n",
        "    )\n",
        "    self.conv3 = nn.Conv2d(\n",
        "        in_channels=128,\n",
        "        out_channels=256,\n",
        "        kernel_size=3,\n",
        "        padding=1\n",
        "    )\n",
        "    self.conv4 = nn.Conv2d(\n",
        "        in_channels=256,\n",
        "        out_channels=256,\n",
        "        kernel_size=3,\n",
        "        padding=1\n",
        "    )\n",
        "    self.conv5 = nn.Conv2d(\n",
        "        in_channels=256,\n",
        "        out_channels=256,\n",
        "        kernel_size=3,\n",
        "        padding=1\n",
        "    )\n",
        "    self.fc6 = nn.Linear(in_features=31*31*256, out_features=4096) # assuming input image size of 256x256. change in_feats for different sample size\n",
        "    self.fc7 = nn.Linear(in_features=4096, out_features=4096)\n",
        "    self.fc8 = nn.Linear(in_features=4096, out_features=num_classes)\n",
        "    self.norm1 = nn.BatchNorm2d(num_features=64)\n",
        "    self.norm2 = nn.BatchNorm2d(num_features=128)\n",
        "    self.dropout = nn.Dropout(0.5)\n",
        "    self.maxpool = nn.MaxPool2d(2)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.flatten = nn.Flatten()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x =self.conv1(x)\n",
        "    x = self.norm1(x)\n",
        "    x = self.maxpool(x)\n",
        "    x = self.relu(x)\n",
        "\n",
        "    x = self.conv2(x)\n",
        "    x = self.norm2(x)\n",
        "    x = self.maxpool(x)\n",
        "    x = self.relu(x)\n",
        "\n",
        "    x = self.conv3(x)\n",
        "    x = self.relu(x)\n",
        "\n",
        "    x = self.conv4(x)\n",
        "    x = self.relu(x)\n",
        "\n",
        "    x = self.conv5(x)\n",
        "    x = self.relu(x)\n",
        "\n",
        "    x = self.flatten(x)\n",
        "\n",
        "    x = self.dropout(self.fc6(x))\n",
        "    x = self.relu(x)\n",
        "\n",
        "    x = self.dropout(self.fc7(x))\n",
        "    x = self.relu(x)\n",
        "\n",
        "    x = self.fc8(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "def training_unsupervised(model, dataloader, criterion, optimizer, device, epochs, model_path):\n",
        "    model = model.to(device)\n",
        "    model.train()\n",
        "    best_loss = torch.inf\n",
        "    for _ in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch_index, (images, _) in enumerate(dataloader):\n",
        "            optimizer.zero_grad()\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, images)\n",
        "            total_loss += loss.item() * images.size(0)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        avg_loss = total_loss / (batch_index+1)\n",
        "        if avg_loss < best_loss:\n",
        "            best_loss = avg_loss\n",
        "            torch.save(model.state_dict(), model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluation(model, dataloader, criterion, device, phase='Validation'):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    ground_truth = []\n",
        "\n",
        "    true_positives = 0\n",
        "    false_positives = 0\n",
        "    false_negatives = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        total_loss = 0\n",
        "        total_samples = 0\n",
        "\n",
        "        for _, (images, labels) in enumerate(dataloader):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            total_loss += loss.item() * images.size(0)\n",
        "            total_samples += images.size(0)\n",
        "\n",
        "            # Convert output probabilities to binary predictions\n",
        "            preds = (torch.sigmoid(outputs) > 0.5).float()\n",
        "\n",
        "            # Update multi-label metrics\n",
        "            true_positives += (preds * labels).sum().item()\n",
        "            false_positives += ((1 - labels) * preds).sum().item()\n",
        "            false_negatives += (labels * (1 - preds)).sum().item()\n",
        "\n",
        "            predictions.extend(preds.cpu().numpy())\n",
        "            ground_truth.extend(labels.cpu().numpy())\n",
        "\n",
        "        # Calculate multi-label metrics\n",
        "        precision = true_positives / (true_positives + false_positives + 1e-10)\n",
        "        recall = true_positives / (true_positives + false_negatives + 1e-10)\n",
        "        f1_score = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
        "\n",
        "        accuracy = true_positives / (true_positives + false_positives + false_negatives + 1e-10)\n",
        "        loss = total_loss / total_samples\n",
        "\n",
        "        print(f'{phase}\\tF1-Score={f1_score:<10.4f}' +\n",
        "              f'\\t\\tLoss= {loss:<10.4f}' +\n",
        "              f'\\t\\tPrecision: {precision:<10.4f}' +\n",
        "              f'\\t\\tRecall: {recall:<10.4f}' +\n",
        "              f'\\t\\tAccuracy: {accuracy:<10.4f}')\n",
        "\n",
        "        return {'loss': loss,\n",
        "                'f1_score': f1_score,\n",
        "                'precision': precision,\n",
        "                'recall': recall,\n",
        "                'accuracy': accuracy,\n",
        "                'ground_truth': ground_truth,\n",
        "                'predictions': predictions}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "def training_supervised(model, train_loader, val_loader, criterion, optimizer, device, epochs, best_model_path):\n",
        "    model = model.to(device)\n",
        "    model.train()\n",
        "    best_loss = torch.inf\n",
        "    best_results = None\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        total_samples = 0\n",
        "\n",
        "        # New variables for multi-label metrics\n",
        "        true_positives = 0\n",
        "        false_positives = 0\n",
        "        false_negatives = 0\n",
        "\n",
        "        for _, (images, labels) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item() * images.size(0)\n",
        "            total_samples += images.size(0)\n",
        "\n",
        "            # Convert output probabilities to binary predictions\n",
        "            preds = (torch.sigmoid(outputs) > 0.5).float()\n",
        "\n",
        "            # Update multi-label metrics\n",
        "            true_positives += (preds * labels).sum().item()\n",
        "            false_positives += ((1 - labels) * preds).sum().item()\n",
        "            false_negatives += (labels * (1 - preds)).sum().item()\n",
        "\n",
        "        # Calculate multi-label metrics\n",
        "        precision = true_positives / (true_positives + false_positives + 1e-10)\n",
        "        recall = true_positives / (true_positives + false_negatives + 1e-10)\n",
        "        f1_score = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
        "\n",
        "        accuracy = true_positives / (true_positives + false_positives + false_negatives + 1e-10)\n",
        "        loss = total_loss / total_samples\n",
        "\n",
        "        print(f'{epoch:<4}\\tTrain\\tF1-Score={f1_score:<10.4f}' +\n",
        "              f'\\t\\tLoss= {loss:<10.4f}' +\n",
        "              f'\\t\\tPrecision: {precision:<10.4f}' +\n",
        "              f'\\t\\tRecall: {recall:<10.4f}' +\n",
        "              f'\\t\\tAccuracy: {accuracy:<10.4f}')\n",
        "\n",
        "        results = evaluation(model, val_loader, criterion, device)\n",
        "\n",
        "        if results['loss'] < best_loss:\n",
        "            torch.save(model, best_model_path)\n",
        "            best_loss = results['loss']\n",
        "            best_results = results\n",
        "\n",
        "        print()\n",
        "\n",
        "    return best_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\P2L Inc\\Documents\\CPSC599A3\\CPSC599-Font-Study\\computer_vision_model.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/P2L%20Inc/Documents/CPSC599A3/CPSC599-Font-Study/computer_vision_model.ipynb#X20sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mSGD(scae_model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mlearning_rate, momentum\u001b[39m=\u001b[39mmomentum, weight_decay\u001b[39m=\u001b[39mweight_decay)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/P2L%20Inc/Documents/CPSC599A3/CPSC599-Font-Study/computer_vision_model.ipynb#X20sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m model_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(models_dir, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSCAE.pt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/P2L%20Inc/Documents/CPSC599A3/CPSC599-Font-Study/computer_vision_model.ipynb#X20sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m training_unsupervised(scae_model, unsupervised_loader, criterion, optimizer, device, epochs, model_path)\n",
            "\u001b[1;32mc:\\Users\\P2L Inc\\Documents\\CPSC599A3\\CPSC599-Font-Study\\computer_vision_model.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/P2L%20Inc/Documents/CPSC599A3/CPSC599-Font-Study/computer_vision_model.ipynb#X20sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     loss \u001b[39m=\u001b[39m criterion(outputs, images)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/P2L%20Inc/Documents/CPSC599A3/CPSC599-Font-Study/computer_vision_model.ipynb#X20sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem() \u001b[39m*\u001b[39m images\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/P2L%20Inc/Documents/CPSC599A3/CPSC599-Font-Study/computer_vision_model.ipynb#X20sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/P2L%20Inc/Documents/CPSC599A3/CPSC599-Font-Study/computer_vision_model.ipynb#X20sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/P2L%20Inc/Documents/CPSC599A3/CPSC599-Font-Study/computer_vision_model.ipynb#X20sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m avg_loss \u001b[39m=\u001b[39m total_loss \u001b[39m/\u001b[39m (batch_index\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    491\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mregister_hook\u001b[39m(\u001b[39mself\u001b[39m, hook):\n\u001b[1;32m--> 492\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Registers a backward hook.\u001b[39;00m\n\u001b[0;32m    493\u001b[0m \n\u001b[0;32m    494\u001b[0m \u001b[39m    The hook will be called every time a gradient with respect to the\u001b[39;00m\n\u001b[0;32m    495\u001b[0m \u001b[39m    Tensor is computed. The hook should have the following signature::\u001b[39;00m\n\u001b[0;32m    496\u001b[0m \n\u001b[0;32m    497\u001b[0m \u001b[39m        hook(grad) -> Tensor or None\u001b[39;00m\n\u001b[0;32m    498\u001b[0m \n\u001b[0;32m    499\u001b[0m \n\u001b[0;32m    500\u001b[0m \u001b[39m    The hook should not modify its argument, but it can optionally return\u001b[39;00m\n\u001b[0;32m    501\u001b[0m \u001b[39m    a new gradient which will be used in place of :attr:`grad`.\u001b[39;00m\n\u001b[0;32m    502\u001b[0m \n\u001b[0;32m    503\u001b[0m \u001b[39m    This function returns a handle with a method ``handle.remove()``\u001b[39;00m\n\u001b[0;32m    504\u001b[0m \u001b[39m    that removes the hook from the module.\u001b[39;00m\n\u001b[0;32m    505\u001b[0m \n\u001b[0;32m    506\u001b[0m \u001b[39m    .. note::\u001b[39;00m\n\u001b[0;32m    507\u001b[0m \u001b[39m        See :ref:`backward-hooks-execution` for more information on how when this hook\u001b[39;00m\n\u001b[0;32m    508\u001b[0m \u001b[39m        is executed, and how its execution is ordered relative to other hooks.\u001b[39;00m\n\u001b[0;32m    509\u001b[0m \n\u001b[0;32m    510\u001b[0m \u001b[39m    Example::\u001b[39;00m\n\u001b[0;32m    511\u001b[0m \n\u001b[0;32m    512\u001b[0m \u001b[39m        >>> v = torch.tensor([0., 0., 0.], requires_grad=True)\u001b[39;00m\n\u001b[0;32m    513\u001b[0m \u001b[39m        >>> h = v.register_hook(lambda grad: grad * 2)  # double the gradient\u001b[39;00m\n\u001b[0;32m    514\u001b[0m \u001b[39m        >>> v.backward(torch.tensor([1., 2., 3.]))\u001b[39;00m\n\u001b[0;32m    515\u001b[0m \u001b[39m        >>> v.grad\u001b[39;00m\n\u001b[0;32m    516\u001b[0m \n\u001b[0;32m    517\u001b[0m \u001b[39m         2\u001b[39;00m\n\u001b[0;32m    518\u001b[0m \u001b[39m         4\u001b[39;00m\n\u001b[0;32m    519\u001b[0m \u001b[39m         6\u001b[39;00m\n\u001b[0;32m    520\u001b[0m \u001b[39m        [torch.FloatTensor of size (3,)]\u001b[39;00m\n\u001b[0;32m    521\u001b[0m \n\u001b[0;32m    522\u001b[0m \u001b[39m        >>> h.remove()  # removes the hook\u001b[39;00m\n\u001b[0;32m    523\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m    524\u001b[0m     \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    525\u001b[0m         \u001b[39mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[39m.\u001b[39mregister_hook, (\u001b[39mself\u001b[39m,), \u001b[39mself\u001b[39m, hook)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgrad\u001b[39m(\n\u001b[0;32m    205\u001b[0m     outputs: _TensorOrTensors,\n\u001b[0;32m    206\u001b[0m     inputs: _TensorOrTensors,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    212\u001b[0m     is_grads_batched: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    213\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]:\n\u001b[0;32m    214\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Computes and returns the sum of gradients of outputs with respect to\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[39m    the inputs.\u001b[39;00m\n\u001b[0;32m    216\u001b[0m \n\u001b[0;32m    217\u001b[0m \u001b[39m    ``grad_outputs`` should be a sequence of length matching ``output``\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[39m    containing the \"vector\" in vector-Jacobian product, usually the pre-computed\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \u001b[39m    gradients w.r.t. each of the outputs. If an output doesn't require_grad,\u001b[39;00m\n\u001b[0;32m    220\u001b[0m \u001b[39m    then the gradient can be ``None``).\u001b[39;00m\n\u001b[0;32m    221\u001b[0m \n\u001b[0;32m    222\u001b[0m \u001b[39m    .. note::\u001b[39;00m\n\u001b[0;32m    223\u001b[0m \n\u001b[0;32m    224\u001b[0m \u001b[39m        If you run any forward ops, create ``grad_outputs``, and/or call ``grad``\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \u001b[39m        in a user-specified CUDA stream context, see\u001b[39;00m\n\u001b[0;32m    226\u001b[0m \u001b[39m        :ref:`Stream semantics of backward passes<bwd-cuda-stream-semantics>`.\u001b[39;00m\n\u001b[0;32m    227\u001b[0m \n\u001b[0;32m    228\u001b[0m \u001b[39m    .. note::\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \n\u001b[0;32m    230\u001b[0m \u001b[39m        ``only_inputs`` argument is deprecated and is ignored now (defaults to ``True``).\u001b[39;00m\n\u001b[0;32m    231\u001b[0m \u001b[39m        To accumulate gradient for other parts of the graph, please use\u001b[39;00m\n\u001b[0;32m    232\u001b[0m \u001b[39m        ``torch.autograd.backward``.\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \n\u001b[0;32m    234\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m    235\u001b[0m \u001b[39m        outputs (sequence of Tensor): outputs of the differentiated function.\u001b[39;00m\n\u001b[0;32m    236\u001b[0m \u001b[39m        inputs (sequence of Tensor): Inputs w.r.t. which the gradient will be\u001b[39;00m\n\u001b[0;32m    237\u001b[0m \u001b[39m            returned (and not accumulated into ``.grad``).\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[39m        grad_outputs (sequence of Tensor): The \"vector\" in the vector-Jacobian product.\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[39m            Usually gradients w.r.t. each output. None values can be specified for scalar\u001b[39;00m\n\u001b[0;32m    240\u001b[0m \u001b[39m            Tensors or ones that don't require grad. If a None value would be acceptable\u001b[39;00m\n\u001b[0;32m    241\u001b[0m \u001b[39m            for all grad_tensors, then this argument is optional. Default: None.\u001b[39;00m\n\u001b[0;32m    242\u001b[0m \u001b[39m        retain_graph (bool, optional): If ``False``, the graph used to compute the grad\u001b[39;00m\n\u001b[0;32m    243\u001b[0m \u001b[39m            will be freed. Note that in nearly all cases setting this option to ``True``\u001b[39;00m\n\u001b[0;32m    244\u001b[0m \u001b[39m            is not needed and often can be worked around in a much more efficient\u001b[39;00m\n\u001b[0;32m    245\u001b[0m \u001b[39m            way. Defaults to the value of ``create_graph``.\u001b[39;00m\n\u001b[0;32m    246\u001b[0m \u001b[39m        create_graph (bool, optional): If ``True``, graph of the derivative will\u001b[39;00m\n\u001b[0;32m    247\u001b[0m \u001b[39m            be constructed, allowing to compute higher order derivative products.\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \u001b[39m            Default: ``False``.\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[39m        allow_unused (bool, optional): If ``False``, specifying inputs that were not\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[39m            used when computing outputs (and therefore their grad is always zero)\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[39m            is an error. Defaults to ``False``.\u001b[39;00m\n\u001b[0;32m    252\u001b[0m \u001b[39m        is_grads_batched (bool, optional): If ``True``, the first dimension of each\u001b[39;00m\n\u001b[0;32m    253\u001b[0m \u001b[39m            tensor in ``grad_outputs`` will be interpreted as the batch dimension.\u001b[39;00m\n\u001b[0;32m    254\u001b[0m \u001b[39m            Instead of computing a single vector-Jacobian product, we compute a\u001b[39;00m\n\u001b[0;32m    255\u001b[0m \u001b[39m            batch of vector-Jacobian products for each \"vector\" in the batch.\u001b[39;00m\n\u001b[0;32m    256\u001b[0m \u001b[39m            We use the vmap prototype feature as the backend to vectorize calls\u001b[39;00m\n\u001b[0;32m    257\u001b[0m \u001b[39m            to the autograd engine so that this computation can be performed in a\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[39m            single call. This should lead to performance improvements when compared\u001b[39;00m\n\u001b[0;32m    259\u001b[0m \u001b[39m            to manually looping and performing backward multiple times. Note that\u001b[39;00m\n\u001b[0;32m    260\u001b[0m \u001b[39m            due to this feature being experimental, there may be performance\u001b[39;00m\n\u001b[0;32m    261\u001b[0m \u001b[39m            cliffs. Please use ``torch._C._debug_only_display_vmap_fallback_warnings(True)``\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \u001b[39m            to show any performance warnings and file an issue on github if warnings exist\u001b[39;00m\n\u001b[0;32m    263\u001b[0m \u001b[39m            for your use case. Defaults to ``False``.\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m    265\u001b[0m     t_outputs \u001b[39m=\u001b[39m cast(Tuple[torch\u001b[39m.\u001b[39mTensor, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m], (outputs,) \u001b[39mif\u001b[39;00m is_tensor_like(outputs) \u001b[39melse\u001b[39;00m \u001b[39mtuple\u001b[39m(outputs))\n\u001b[0;32m    266\u001b[0m     t_inputs \u001b[39m=\u001b[39m cast(Tuple[torch\u001b[39m.\u001b[39mTensor, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m], (inputs,) \u001b[39mif\u001b[39;00m is_tensor_like(inputs) \u001b[39melse\u001b[39;00m \u001b[39mtuple\u001b[39m(inputs))\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "models_dir = 'models'\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "# Train the unsupervised sub-network\n",
        "# Hyperparameters\n",
        "learning_rate = 0.01\n",
        "momentum = 0.9\n",
        "weight_decay = 5e-4\n",
        "epochs = 10\n",
        "\n",
        "scae_model = SCAE(num_channels=3)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.SGD(scae_model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
        "\n",
        "model_path = os.path.join(models_dir, f\"SCAE.pt\")\n",
        "training_unsupervised(scae_model, unsupervised_loader, criterion, optimizer, device, epochs, model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'DeepFont' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\P2L Inc\\Documents\\CPSC599A3\\CPSC599-Font-Study\\computer_vision_model.ipynb Cell 16\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/P2L%20Inc/Documents/CPSC599A3/CPSC599-Font-Study/computer_vision_model.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Train the supervised sub-network\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/P2L%20Inc/Documents/CPSC599A3/CPSC599-Font-Study/computer_vision_model.ipynb#X21sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m supervised_model \u001b[39m=\u001b[39m DeepFont(num_channels\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, num_classes\u001b[39m=\u001b[39m\u001b[39m52\u001b[39m) \u001b[39m# one class per letter (case-sensitive)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/P2L%20Inc/Documents/CPSC599A3/CPSC599-Font-Study/computer_vision_model.ipynb#X21sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Import the convolutional layers of the SCAE as conv1 and conv2\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/P2L%20Inc/Documents/CPSC599A3/CPSC599-Font-Study/computer_vision_model.ipynb#X21sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m scae_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(models_dir, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSCAE.pt\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'DeepFont' is not defined"
          ]
        }
      ],
      "source": [
        "# Train the supervised sub-network\n",
        "supervised_model = DeepFont(num_channels=3, num_classes=52) # one class per letter (case-sensitive)\n",
        "\n",
        "# Import the convolutional layers of the SCAE as conv1 and conv2\n",
        "scae_path = os.path.join(models_dir, f\"SCAE.pt\")\n",
        "supervised_model.load_state_dict(torch.load(scae_path), strict=False)\n",
        "\n",
        "# Freeze the convolutional layers from SCAE\n",
        "for param in supervised_model.conv1.parameters():\n",
        "    param.requires_grad = False\n",
        "for param in supervised_model.conv2.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "i=3\n",
        "font_name = models[i]\n",
        "print(font_name)\n",
        "    \n",
        "train_loader = train_loaders[i]\n",
        "val_loader = val_loaders[i]\n",
        "best_model_path = os.path.join(models_dir, f\"{font_name}_model.pt\")\n",
        "best_results = training_supervised(supervised_model, train_loader, val_loader, criterion, optimizer, device, epochs, best_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Testing our models\n",
        "\n",
        "models_dir = 'models'\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "for i in range(len(models)):\n",
        "    font_name = models[i]\n",
        "    print(font_name)\n",
        "    model_path = os.path.join(models_dir, f\"{font_name}_model.pt\")\n",
        "    model = torch.load(model_path, map_location=device)\n",
        "    results = evaluation(model, test_loader, criterion, device, 'Test')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
